#!/bin/bash
# ============================================
# ðŸš€ RUN BRAZILIAN PIPELINE
# Executa pipeline completo com configuraÃ§Ã£o otimizada
# 
# ConfiguraÃ§Ã£o:
# - 5 Workers (10 cores, 15GB RAM total)
# - PartiÃ§Ãµes: 128MB por lote
# - spark.sql.files.maxPartitionBytes = 128MB
# - PartiÃ§Ãµes = 128MB * 10 cores = ~1.28GB por rodada
# ============================================

set -e

SPARK_MASTER="spark://spark-master:7077"
JARS="/jars/hadoop-aws-3.3.4.jar,/jars/aws-java-sdk-bundle-1.12.262.jar,/jars/postgresql-42.7.4.jar"
CLASSPATH="/jars/hadoop-aws-3.3.4.jar:/jars/aws-java-sdk-bundle-1.12.262.jar:/jars/postgresql-42.7.4.jar"

# ConfiguraÃ§Ãµes de particionamento otimizado
# 128MB por partiÃ§Ã£o, distribuÃ­do entre todos os cores
SPARK_CONF=(
    "--conf" "spark.sql.files.maxPartitionBytes=134217728"
    "--conf" "spark.sql.shuffle.partitions=20"
    "--conf" "spark.default.parallelism=10"
    "--conf" "spark.executor.extraClassPath=$CLASSPATH"
    "--conf" "spark.driver.extraClassPath=$CLASSPATH"
    "--conf" "spark.sql.adaptive.enabled=true"
    "--conf" "spark.sql.adaptive.coalescePartitions.enabled=true"
    "--conf" "spark.sql.adaptive.coalescePartitions.minPartitionSize=67108864"
)

echo "============================================"
echo "ðŸš€ BRAZILIAN FRAUD DETECTION PIPELINE"
echo "============================================"
echo "ðŸ“Š ConfiguraÃ§Ã£o:"
echo "   â€¢ Workers: 5 (2 cores cada = 10 cores)"
echo "   â€¢ MemÃ³ria: 3GB por executor"
echo "   â€¢ PartiÃ§Ãµes: 128MB por lote"
echo "   â€¢ Shuffle partitions: 20"
echo "============================================"
echo ""

# FunÃ§Ã£o para executar job
run_job() {
    local job_name=$1
    local job_path=$2
    
    echo "ðŸ”„ Executando: $job_name"
    echo "   Arquivo: $job_path"
    
    docker exec fraud_spark_master /opt/spark/bin/spark-submit \
        --master $SPARK_MASTER \
        --deploy-mode client \
        --executor-memory 3g \
        --total-executor-cores 10 \
        --jars $JARS \
        "${SPARK_CONF[@]}" \
        $job_path 2>&1 | grep -vE "^[0-9]{2}/[0-9]{2}/[0-9]{2}|INFO|WARN"
    
    echo ""
}

# Pipeline stages
case "${1:-all}" in
    bronze)
        run_job "ðŸ¥‰ BRONZE LAYER" "/jobs/production/bronze_brazilian.py"
        ;;
    silver)
        run_job "ðŸ¥ˆ SILVER LAYER" "/jobs/production/silver_brazilian.py"
        ;;
    gold)
        run_job "ðŸ¥‡ GOLD LAYER" "/jobs/production/gold_brazilian.py"
        ;;
    postgres)
        run_job "ðŸ“¦ LOAD TO POSTGRES" "/jobs/production/load_to_postgres.py"
        ;;
    all)
        echo "ðŸ”„ Executando pipeline completo..."
        echo ""
        run_job "ðŸ¥‰ BRONZE LAYER" "/jobs/production/bronze_brazilian.py"
        run_job "ðŸ¥ˆ SILVER LAYER" "/jobs/production/silver_brazilian.py"
        run_job "ðŸ¥‡ GOLD LAYER" "/jobs/production/gold_brazilian.py"
        run_job "ðŸ“¦ LOAD TO POSTGRES" "/jobs/production/load_to_postgres.py"
        ;;
    *)
        echo "Uso: $0 {bronze|silver|gold|postgres|all}"
        exit 1
        ;;
esac

echo "============================================"
echo "âœ… Pipeline concluÃ­do!"
echo "============================================"
