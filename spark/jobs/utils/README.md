# üîß Utils - Scripts Utilit√°rios

## üìã Vis√£o Geral

Scripts auxiliares para **verifica√ß√£o, debug e an√°lise** dos dados.
N√£o fazem parte do pipeline principal, mas s√£o √∫teis para desenvolvimento.

## üìÅ Arquivos

| Arquivo | Descri√ß√£o | Uso |
|---------|-----------|-----|
| `check_flags.py` | Verifica flags de fraude no Silver | Debug/An√°lise |
| `check_gps.py` | Verifica coordenadas GPS dos customers | Valida√ß√£o |

## üéØ Detalhes dos Scripts

### check_flags.py

**Prop√≥sito**: Verificar se as flags de fraude est√£o sendo calculadas corretamente.

**O que faz**:
- L√™ dados da camada Silver
- Mostra estat√≠sticas das flags
- Identifica transa√ß√µes suspeitas

**Quando usar**:
- Ap√≥s rodar `medallion_silver.py`
- Para validar novas regras de fraude
- Debug quando alertas parecem incorretos

```python
# Exemplo de output:
+-------------------+-------+
| flag              | count |
+-------------------+-------+
| is_high_amount    |  1234 |
| is_unusual_hour   |   567 |
| is_foreign        |   890 |
| is_cloning_suspect|     0 |
+-------------------+-------+
```

### check_gps.py

**Prop√≥sito**: Validar coordenadas GPS dos clientes.

**O que faz**:
- L√™ dados de customers
- Verifica latitude/longitude v√°lidas
- Identifica coordenadas fora do Brasil

**Quando usar**:
- Validar dados de entrada
- Antes de implementar regra de dist√¢ncia
- Debug de problemas geogr√°ficos

## üñ•Ô∏è Como Executar

### No Cluster Spark

```bash
docker exec -it spark-master bash

# Check Flags
spark-submit \
  --master spark://spark-master:7077 \
  --jars /jars/hadoop-aws-3.3.4.jar,/jars/aws-java-sdk-bundle-1.12.262.jar \
  /spark/jobs/utils/check_flags.py

# Check GPS
spark-submit \
  --master spark://spark-master:7077 \
  --jars /jars/hadoop-aws-3.3.4.jar,/jars/aws-java-sdk-bundle-1.12.262.jar \
  /spark/jobs/utils/check_gps.py
```

### Execu√ß√£o Local

```bash
spark-submit \
  --master local[*] \
  --jars /path/to/jars/hadoop-aws-3.3.4.jar,/path/to/jars/aws-java-sdk-bundle-1.12.262.jar \
  check_flags.py
```

## üîç Interpretando os Resultados

### check_flags.py

| Flag | Significado | Esperado |
|------|-------------|----------|
| `is_high_amount` | Valor > R$5.000 | 5-15% |
| `is_unusual_hour` | 00:00 - 06:00 | 10-20% |
| `is_foreign` | Pa√≠s != Brasil | 1-5% |
| `is_cloning_suspect` | Transa√ß√£o ap√≥s < 5min | ~0% (dados teste) |
| `is_suspicious_category` | Eletr√¥nicos/Passagens | 10-15% |
| `is_online_high_value` | Online + > R$1.000 | 3-8% |

### check_gps.py

```
‚úÖ GPS v√°lido: latitude entre -33.75 e 5.27 (Brasil)
‚úÖ GPS v√°lido: longitude entre -73.99 e -34.79 (Brasil)
‚ö†Ô∏è GPS suspeito: coordenadas fora do Brasil
‚ùå GPS inv√°lido: valores nulos ou zerados
```

## üõ†Ô∏è Criando Novos Utils

Template para criar um novo script de verifica√ß√£o:

```python
from pyspark.sql import SparkSession

def main():
    spark = SparkSession.builder \
        .appName("CheckNomeDoCheck") \
        .getOrCreate()
    
    # Configurar MinIO
    spark.sparkContext._jsc.hadoopConfiguration().set("fs.s3a.endpoint", "http://minio:9000")
    spark.sparkContext._jsc.hadoopConfiguration().set("fs.s3a.access.key", "minioadmin")
    spark.sparkContext._jsc.hadoopConfiguration().set("fs.s3a.secret.key", "minioadmin")
    spark.sparkContext._jsc.hadoopConfiguration().set("fs.s3a.path.style.access", "true")
    
    # Ler dados
    df = spark.read.parquet("s3a://lakehouse/silver/transactions")
    
    # Suas verifica√ß√µes aqui
    print("=== Verifica√ß√£o XYZ ===")
    df.groupBy("coluna").count().show()
    
    spark.stop()

if __name__ == "__main__":
    main()
```

## üìù Sugest√µes de Novos Utils

- [ ] `check_duplicates.py` - Encontrar transa√ß√µes duplicadas
- [ ] `check_schema.py` - Validar schema dos dados
- [ ] `check_nulls.py` - Identificar campos com muitos nulls
- [ ] `check_outliers.py` - Detectar valores discrepantes
- [ ] `compare_layers.py` - Comparar contagens Bronze/Silver/Gold
