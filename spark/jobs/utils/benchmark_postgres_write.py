"""
ğŸ“Š BENCHMARK: JDBC Write Performance
Compara escrita ATUAL vs OTIMIZADA para PostgreSQL

Testa:
1. Escrita SEM otimizaÃ§Ã£o (baseline atual)
2. Escrita COM repartition + batchsize + numPartitions

Usa tabela temporÃ¡ria para nÃ£o afetar dados de produÃ§Ã£o.
"""

import sys
sys.path.insert(0, '/jobs')

import time
from pyspark.sql import SparkSession
from pyspark.sql.functions import col
from config import POSTGRES_URL, POSTGRES_PROPERTIES, GOLD_PATH, apply_s3a_configs

# ============================================
# CONFIGURAÃ‡ÃƒO DO BENCHMARK
# ============================================
SAMPLE_SIZE = 10_000_000  # 10M registros
TABLE_BASELINE = "benchmark_baseline"
TABLE_OPTIMIZED = "benchmark_optimized"

print("=" * 70)
print("ğŸ“Š BENCHMARK: JDBC Write Performance")
print("=" * 70)
print(f"ğŸ¯ Amostra: {SAMPLE_SIZE:,} registros")
print(f"ğŸ“¦ Tabelas: {TABLE_BASELINE} vs {TABLE_OPTIMIZED}")
print("=" * 70)

# ============================================
# INICIALIZAR SPARK
# ============================================
spark = apply_s3a_configs(
    SparkSession.builder.appName("Benchmark_Postgres_Write")
).getOrCreate()

spark.sparkContext.setLogLevel("WARN")

# ============================================
# CARREGAR DADOS DO GOLD
# ============================================
print("\nğŸ“‚ Carregando dados do Gold Layer...")
start_load = time.time()

df_full = spark.read.parquet(f"{GOLD_PATH}/fraud_detection")

# Selecionar mesmos campos do load_to_postgres.py
df_tx = df_full.select(
    "transaction_id",
    "customer_id",
    "timestamp_dt",
    "tx_date",
    "tx_year",
    "tx_month",
    "tx_hour",
    "tipo",
    col("valor").alias("amount"),
    "canal",
    "merchant_name",
    "merchant_category",
    "mcc_code",
    "mcc_risk_level",
    "bandeira",
    "entrada_cartao",
    "status",
    "motivo_recusa",
    "fraud_score",
    "fraud_score_category",
    "is_fraud",
    "fraud_type",
    "risk_points",
    "risk_level",
    "requires_review",
    "periodo_dia",
    "faixa_valor",
    "is_weekend",
    "_gold_timestamp"
)

# Limitar a amostra
df_sample = df_tx.limit(SAMPLE_SIZE)

# Cache para reutilizar nos testes
df_sample.cache()
actual_count = df_sample.count()
elapsed_load = time.time() - start_load

print(f"âœ… {actual_count:,} registros carregados em {elapsed_load:.1f}s")
print(f"ğŸ“Š PartiÃ§Ãµes atuais: {df_sample.rdd.getNumPartitions()}")

# ============================================
# TESTE 1: BASELINE (sem otimizaÃ§Ã£o)
# ============================================
print("\n" + "=" * 70)
print("ğŸ”µ TESTE 1: BASELINE (configuraÃ§Ã£o atual)")
print("=" * 70)
print("   - Sem repartition")
print("   - batchsize padrÃ£o (1000)")
print("   - Escrita direta")

start_baseline = time.time()

df_sample.write \
    .mode("overwrite") \
    .jdbc(POSTGRES_URL, TABLE_BASELINE, properties=POSTGRES_PROPERTIES)

elapsed_baseline = time.time() - start_baseline
throughput_baseline = actual_count / elapsed_baseline

print(f"\nâ±ï¸  Tempo: {elapsed_baseline:.1f}s")
print(f"ğŸš€ Throughput: {throughput_baseline:,.0f} registros/s")

# ============================================
# TESTE 2: OTIMIZADO (com boas prÃ¡ticas)
# ============================================
print("\n" + "=" * 70)
print("ğŸŸ¢ TESTE 2: OTIMIZADO (boas prÃ¡ticas)")
print("=" * 70)

# ConfiguraÃ§Ãµes otimizadas
NUM_PARTITIONS = 16  # ConexÃµes paralelas ao PostgreSQL
BATCH_SIZE = 10000   # Linhas por INSERT

print(f"   - repartition({NUM_PARTITIONS})")
print(f"   - batchsize: {BATCH_SIZE}")
print(f"   - rewriteBatchedInserts: true")

# Properties otimizadas
optimized_properties = POSTGRES_PROPERTIES.copy()
optimized_properties["batchsize"] = str(BATCH_SIZE)
optimized_properties["rewriteBatchedInserts"] = "true"

start_optimized = time.time()

df_sample.repartition(NUM_PARTITIONS).write \
    .mode("overwrite") \
    .option("numPartitions", NUM_PARTITIONS) \
    .option("truncate", "true") \
    .jdbc(POSTGRES_URL, TABLE_OPTIMIZED, properties=optimized_properties)

elapsed_optimized = time.time() - start_optimized
throughput_optimized = actual_count / elapsed_optimized

print(f"\nâ±ï¸  Tempo: {elapsed_optimized:.1f}s")
print(f"ğŸš€ Throughput: {throughput_optimized:,.0f} registros/s")

# ============================================
# COMPARATIVO
# ============================================
print("\n" + "=" * 70)
print("ğŸ“Š RESULTADO DO BENCHMARK")
print("=" * 70)

speedup = elapsed_baseline / elapsed_optimized if elapsed_optimized > 0 else 0
improvement = ((throughput_optimized - throughput_baseline) / throughput_baseline * 100) if throughput_baseline > 0 else 0

print(f"""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MÃ©trica             â”‚ BASELINE        â”‚ OTIMIZADO       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Registros           â”‚ {actual_count:>15,} â”‚ {actual_count:>15,} â”‚
â”‚ Tempo (s)           â”‚ {elapsed_baseline:>15.1f} â”‚ {elapsed_optimized:>15.1f} â”‚
â”‚ Throughput (reg/s)  â”‚ {throughput_baseline:>15,.0f} â”‚ {throughput_optimized:>15,.0f} â”‚
â”‚ PartiÃ§Ãµes           â”‚ {df_sample.rdd.getNumPartitions():>15} â”‚ {NUM_PARTITIONS:>15} â”‚
â”‚ Batch Size          â”‚ {1000:>15} â”‚ {BATCH_SIZE:>15} â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ğŸ¯ SPEEDUP: {speedup:.2f}x mais rÃ¡pido
ğŸ“ˆ MELHORIA: {improvement:+.1f}% throughput
""")

# Limpar cache
df_sample.unpersist()

# ============================================
# LIMPAR TABELAS TEMPORÃRIAS
# ============================================
print("ğŸ§¹ Limpando tabelas temporÃ¡rias...")

try:
    from py4j.java_gateway import java_import
    java_import(spark._jvm, "java.sql.DriverManager")
    
    conn = spark._jvm.DriverManager.getConnection(
        POSTGRES_URL,
        POSTGRES_PROPERTIES["user"],
        POSTGRES_PROPERTIES["password"]
    )
    stmt = conn.createStatement()
    stmt.execute(f"DROP TABLE IF EXISTS {TABLE_BASELINE}")
    stmt.execute(f"DROP TABLE IF EXISTS {TABLE_OPTIMIZED}")
    stmt.close()
    conn.close()
    print("âœ… Tabelas temporÃ¡rias removidas")
except Exception as e:
    print(f"âš ï¸  Erro ao limpar: {e}")
    print("   Execute: DROP TABLE IF EXISTS benchmark_baseline, benchmark_optimized;")

print("\n" + "=" * 70)
print("âœ… BENCHMARK CONCLUÃDO!")
print("=" * 70)

spark.stop()
