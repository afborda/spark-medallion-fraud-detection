"""
Fraud Detection - Regras de NegÃ³cio
Aplica regras para identificar transaÃ§Ãµes suspeitas
"""


from pyspark.sql import SparkSession
from pyspark.sql.functions import col, when, hour, to_timestamp
from datetime import date


# caminhos

SILVER_PATH = "data/silver"
FRAUD_PATH = "data/gold/fraud_detection"
PROCESS_DATE = date.today().isoformat()

print("=" * 50)
print("ðŸš¨ FRAUD DETECTION - Regras de NegÃ³cio")
print("=" * 50)
print(f"ðŸ“‚ Origem: {SILVER_PATH}")
print(f"ðŸ“‚ Destino: {FRAUD_PATH}")
print(f"ðŸ“… Data: {PROCESS_DATE}")
print("=" * 50)



print ("ðŸš€ Iniciando Spark Session...")
spark = SparkSession.builder \
	.appName("Fraud Detection - Business Rules") \
	.getOrCreate()

	
def apply_fraud_rules(df):
	"""
	Aplicar regras de detecÃ§Ã£o de fraude 
	regras:
	Valor alto: transÃ§oes > R$ 1000
	horario suspeito: entre 2h e 5h
	CombinaÃ§Ã£o: valor alto + horario suspeito = Alto Risco
	"""

	print("\nðŸš¨ Aplicando regras de detecÃ§Ã£o de fraude...")

	#Converter timestamp para extrair hora
	df_wirh_hour = df.withColumn("transaction_hour", hour(to_timestamp(col("timestamp"))))

	#Regra 1: Valor alto
	df_rules = df_wirh_hour.withColumn(
		"high_value",
		when(col("amount") > 1000, True).otherwise(False)
	)

	#Regra 2 : HorÃ¡rio suspeito (2h - 5H)
	df_rules = df_rules.withColumn(
		"suspicious_hour",
		when((col("transaction_hour") >= 2) &
			(col("transaction_hour") <= 5), True
			).otherwise(False)
		)
	#Regra 3: CombnacÃ£o de ambas as regras
	df_rules = df_rules.withColumn(
		"risk_level",
		when((col("high_value") == True) & (col("suspicious_hour") ==True),
		"Alto Risco"
		).when((col("high_value") == True) | (col("suspicious_hour") == True),
		"Risco MÃ©dio"
		).otherwise("Baixo Risco")
	)

	return df_rules 

# 1 Ler dados do silver
print ("ðŸš¨ Carregando dados de transaÃ§Ãµes do Silver Layer...")
df_transactions = spark.read.parquet(f"{SILVER_PATH}/transactions")
print(f"âœ… Dados carregados. Registros: {df_transactions.count()}")

# 2 Aplicar regras de fraude
df_flagged  = apply_fraud_rules(df_transactions)

# 3 mostrar estatÃ­sticas
print("\nðŸ“Š EstatÃ­sticas de risco de fraude:")
df_flagged.groupBy("risk_level").count().show()

# 4 Mostrar  exemplo de cada nivel
print("\nðŸ”´ Exemplos ALTO RISCO:")
df_flagged.filter(col("risk_level") == "Alto Risco") \
	.select("transaction_id", "amount", "transaction_hour", "risk_level") \
	.show(5)

print("\nðŸŸ  Exemplos RISCO MÃ‰DIO:")
df_flagged.filter(col("risk_level") == "Risco MÃ©dio") \
	.select("transaction_id", "amount", "transaction_hour", "risk_level") \
	.show(5)
	
# 5 Salvar resultados 

print(f"\nðŸš¨ Salvando em {FRAUD_PATH}... ")
df_flagged.write \
	.mode("overwrite") \
	.partitionBy("risk_level") \
	.parquet(FRAUD_PATH)

print("\n âœ… Fraudes detectados com sucesso!")
print(f"   ðŸ“ Dados salvos em: {FRAUD_PATH}")

spark.stop()
print("\nðŸš€ Spark Session finalizada.")