"""
Batch Job - Kafka para PostgreSQL
L√™ dados do Kafka e carrega no PostgreSQL (modo batch simples)
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    from_json, col, when, abs as spark_abs, sqrt, pow as spark_pow,
    current_timestamp, round as spark_round, lit
)
from pyspark.sql.types import (
    StructType, StructField, StringType, DoubleType, 
    BooleanType, LongType, IntegerType
)

# Schema das transa√ß√µes
transaction_schema = StructType([
    StructField("transaction_id", StringType(), True),
    StructField("customer_id", StringType(), True),
    StructField("amount", DoubleType(), True),
    StructField("merchant", StringType(), True),
    StructField("category", StringType(), True),
    StructField("transaction_hour", DoubleType(), True),
    StructField("day_of_week", StringType(), True),
    StructField("customer_home_state", StringType(), True),
    StructField("purchase_state", StringType(), True),
    StructField("purchase_city", StringType(), True),
    StructField("purchase_latitude", DoubleType(), True),
    StructField("purchase_longitude", DoubleType(), True),
    StructField("device_latitude", DoubleType(), True),
    StructField("device_longitude", DoubleType(), True),
    StructField("device_id", StringType(), True),
    StructField("ip_address", StringType(), True),
    StructField("payment_method", StringType(), True),
    StructField("card_brand", StringType(), True),
    StructField("installments", IntegerType(), True),
    StructField("had_travel_purchase_last_12m", BooleanType(), True),
    StructField("is_first_purchase_in_state", BooleanType(), True),
    StructField("transactions_last_24h", DoubleType(), True),
    StructField("avg_transaction_amount_30d", DoubleType(), True),
    StructField("is_international", BooleanType(), True),
    StructField("is_online", BooleanType(), True),
    StructField("is_fraud", BooleanType(), True),
    StructField("timestamp", LongType(), True)
])

def main():
    print("=" * 60)
    print("üöÄ BATCH: KAFKA ‚Üí POSTGRESQL")
    print("=" * 60)
    
    spark = SparkSession.builder \
        .appName("Kafka_to_PostgreSQL_Batch") \
        .getOrCreate()
    
    spark.sparkContext.setLogLevel("WARN")
    
    # Ler do Kafka (modo batch)
    print("üì° Lendo dados do Kafka...")
    df_kafka = spark.read \
        .format("kafka") \
        .option("kafka.bootstrap.servers", "fraud_kafka:9092") \
        .option("subscribe", "transactions") \
        .option("startingOffsets", "earliest") \
        .option("endingOffsets", "latest") \
        .load()
    
    total_kafka = df_kafka.count()
    print(f"‚úÖ {total_kafka} mensagens encontradas no Kafka")
    
    if total_kafka == 0:
        print("‚ö†Ô∏è Nenhum dado no Kafka!")
        spark.stop()
        return
    
    # Parsear JSON
    df_transactions = df_kafka \
        .selectExpr("CAST(value AS STRING) as json_value") \
        .select(from_json(col("json_value"), transaction_schema).alias("data")) \
        .select("data.*") \
        .filter(col("transaction_id").isNotNull())
    
    print(f"üìä {df_transactions.count()} transa√ß√µes v√°lidas")
    
    # Aplicar regras de fraude
    print("üîç Aplicando regras de detec√ß√£o de fraude...")
    
    df_processed = df_transactions \
        .withColumn("amount_clean", 
            when(col("amount") < 0, spark_abs(col("amount"))).otherwise(col("amount"))) \
        .withColumn("distance_gps",
            spark_round(sqrt(
                spark_pow(col("device_latitude") - col("purchase_latitude"), 2) +
                spark_pow(col("device_longitude") - col("purchase_longitude"), 2)
            ), 4)) \
        .withColumn("is_cross_state",
            when(col("customer_home_state") != col("purchase_state"), True).otherwise(False)) \
        .withColumn("is_night",
            when(col("transaction_hour") < 6, True).otherwise(False)) \
        .withColumn("is_high_value",
            when(col("amount") > col("avg_transaction_amount_30d") * 3, True).otherwise(False))
    
    # Calcular Fraud Score
    df_scored = df_processed.withColumn("fraud_score",
        (when(col("is_cross_state"), 15).otherwise(0) +
         when(col("is_night"), 10).otherwise(0) +
         when(col("is_high_value"), 20).otherwise(0) +
         when(col("distance_gps") > 5, 25).otherwise(0) +
         when((col("is_cross_state")) & (col("had_travel_purchase_last_12m") == False), 30).otherwise(0) +
         when(col("is_first_purchase_in_state"), 10).otherwise(0) +
         when(col("is_international"), 15).otherwise(0) +
         when(col("transactions_last_24h") > 5, 15).otherwise(0))
    )
    
    # Risk Level
    df_final = df_scored.withColumn("risk_level",
        when(col("fraud_score") >= 70, "CR√çTICO")
        .when(col("fraud_score") >= 50, "ALTO")
        .when(col("fraud_score") >= 30, "M√âDIO")
        .when(col("fraud_score") >= 15, "BAIXO")
        .otherwise("NORMAL")
    )
    
    # Mostrar estat√≠sticas
    print("\nüìà ESTAT√çSTICAS DE RISCO:")
    df_final.groupBy("risk_level").count().orderBy("count", ascending=False).show()
    
    # Preparar para PostgreSQL
    df_to_postgres = df_final.select(
        col("transaction_id"),
        col("customer_id"),
        col("amount_clean").alias("amount"),
        col("merchant"),
        col("category"),
        col("fraud_score").cast("integer"),
        col("risk_level"),
        col("is_fraud")
    )
    
    # Salvar no PostgreSQL
    print("üíæ Salvando no PostgreSQL...")
    
    postgres_url = "jdbc:postgresql://fraud_postgres:5432/fraud_db"
    postgres_props = {
        "user": "fraud_user",
        "password": "fraud_password@@!!_2",
        "driver": "org.postgresql.Driver"
    }
    
    df_to_postgres.write \
        .jdbc(postgres_url, "transactions", mode="append", properties=postgres_props)
    
    print(f"‚úÖ {df_to_postgres.count()} transa√ß√µes salvas na tabela 'transactions'!")
    
    # Alertas de fraude (ALTO e CR√çTICO)
    df_alerts = df_final.filter(col("risk_level").isin("ALTO", "CR√çTICO")) \
        .select(
            col("transaction_id"),
            col("customer_id"),
            col("amount_clean").alias("amount"),
            col("merchant"),
            col("fraud_score").cast("integer"),
            col("risk_level"),
            col("is_fraud"),
            col("customer_home_state"),
            col("purchase_state")
        )
    
    alerts_count = df_alerts.count()
    if alerts_count > 0:
        df_alerts.write \
            .jdbc(postgres_url, "fraud_alerts", mode="append", properties=postgres_props)
        print(f"üö® {alerts_count} ALERTAS de fraude salvos!")
    
    print("\n" + "=" * 60)
    print("‚úÖ PROCESSAMENTO CONCLU√çDO!")
    print("=" * 60)
    
    spark.stop()

if __name__ == "__main__":
    main()
