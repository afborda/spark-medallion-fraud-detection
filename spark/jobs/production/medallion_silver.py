"""
ğŸ¥ˆ SILVER LAYER - Bronze â†’ Silver (MinIO)
Limpeza e enriquecimento dos dados
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import (
    from_json, col, when, abs as spark_abs, sqrt, pow as spark_pow,
    current_timestamp, round as spark_round,
    # NOVOS IMPORTS para Window Functions:
    lag      # Pegar valor da linha anterior (como explicado!)
)
from pyspark.sql.window import Window  # Para criar janelas de anÃ¡lise
from pyspark.sql.types import (
    StructType, StructField, StringType, DoubleType, 
    BooleanType, LongType, IntegerType
)

print("=" * 60)
print("ğŸ¥ˆ SILVER LAYER - Bronze â†’ Silver")
print("=" * 60)

# Schema das transaÃ§Ãµes
transaction_schema = StructType([
    StructField("transaction_id", StringType(), True),
    StructField("customer_id", StringType(), True),
    StructField("amount", DoubleType(), True),
    StructField("merchant", StringType(), True),
    StructField("category", StringType(), True),
    StructField("transaction_hour", DoubleType(), True),
    StructField("day_of_week", StringType(), True),
    StructField("customer_home_state", StringType(), True),
    StructField("purchase_state", StringType(), True),
    StructField("purchase_city", StringType(), True),
    StructField("purchase_latitude", DoubleType(), True),
    StructField("purchase_longitude", DoubleType(), True),
    StructField("device_latitude", DoubleType(), True),
    StructField("device_longitude", DoubleType(), True),
    StructField("device_id", StringType(), True),
    StructField("ip_address", StringType(), True),
    StructField("payment_method", StringType(), True),
    StructField("card_brand", StringType(), True),
    StructField("installments", IntegerType(), True),
    StructField("had_travel_purchase_last_12m", BooleanType(), True),
    StructField("is_first_purchase_in_state", BooleanType(), True),
    StructField("transactions_last_24h", DoubleType(), True),
    StructField("avg_transaction_amount_30d", DoubleType(), True),
    StructField("is_international", BooleanType(), True),
    StructField("is_online", BooleanType(), True),
    StructField("is_fraud", BooleanType(), True),
    StructField("timestamp", LongType(), True)
])

# JARs
JARS_PATH = "/jars"
HADOOP_AWS = f"{JARS_PATH}/hadoop-aws-3.3.4.jar"
AWS_SDK = f"{JARS_PATH}/aws-java-sdk-bundle-1.12.262.jar"
JARS = f"{HADOOP_AWS},{AWS_SDK}"
CLASSPATH = f"{HADOOP_AWS}:{AWS_SDK}"

spark = SparkSession.builder \
    .appName("Silver_Bronze_to_Silver") \
    .config("spark.jars", JARS) \
    .config("spark.driver.extraClassPath", CLASSPATH) \
    .config("spark.executor.extraClassPath", CLASSPATH) \
    .config("spark.hadoop.fs.s3a.endpoint", "http://minio:9000") \
    .config("spark.hadoop.fs.s3a.access.key", "minioadmin") \
    .config("spark.hadoop.fs.s3a.secret.key", "minioadmin123@@!!_2") \
    .config("spark.hadoop.fs.s3a.path.style.access", "true") \
    .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
    .config("spark.hadoop.fs.s3a.connection.ssl.enabled", "false") \
    .getOrCreate()

spark.sparkContext.setLogLevel("WARN")

# Ler Bronze
bronze_path = "s3a://fraud-data/medallion/bronze/transactions"
print(f"ğŸ“‚ Lendo Bronze: {bronze_path}")

df_bronze = spark.read.parquet(bronze_path)
total_bronze = df_bronze.count()
print(f"âœ… {total_bronze:,} registros no Bronze")

# Parsear JSON
print("ğŸ”„ Parseando JSON e limpando dados...")


df_parsed = df_bronze \
    .select(from_json(col("raw_json"), transaction_schema).alias("data")) \
    .select("data.*") \
    .filter(col("transaction_id").isNotNull())

# ============================================================
# REGRA 1: CLONAGEM DE CARTÃƒO (Window Function)
# ============================================================
# 
# CONCEITO: Detectar quando o MESMO cartÃ£o Ã© usado em locais
# geograficamente distantes em um curto perÃ­odo de tempo.
# Isso Ã© IMPOSSÃVEL fisicamente - indica clonagem!
#
# PASSO 1: Criar a "janela" - agrupa por cliente, ordena por tempo
# Ã‰ como criar uma pasta para cada cliente com suas transaÃ§Ãµes em ordem
#
print("ğŸ” Aplicando Window Functions para detectar clonagem...")

window_por_cliente = Window.partitionBy("customer_id").orderBy("timestamp")
#                          â†‘                            â†‘
#                          â”‚                            â””â”€ Ordena por tempo (mais antiga primeiro)
#                          â””â”€ Cada cliente Ã© uma "pasta" separada

# PASSO 2: Usar lag() para pegar dados da transaÃ§Ã£o ANTERIOR
# Ã‰ como perguntar: "Qual foi a compra anterior deste cliente?"
df_with_prev = df_parsed \
    .withColumn("prev_timestamp", lag("timestamp", 1).over(window_por_cliente)) \
    .withColumn("prev_latitude", lag("purchase_latitude", 1).over(window_por_cliente)) \
    .withColumn("prev_longitude", lag("purchase_longitude", 1).over(window_por_cliente)) \
    .withColumn("prev_state", lag("purchase_state", 1).over(window_por_cliente))
#               â†‘              â†‘                    â†‘        â†‘
#               â”‚              â”‚                    â”‚        â””â”€ Aplica na janela que criamos
#               â”‚              â”‚                    â””â”€ 1 = uma linha para trÃ¡s
#               â”‚              â””â”€ Coluna que quero da linha anterior
#               â””â”€ Nome da nova coluna

# PASSO 3: Calcular diferenÃ§a de tempo (em minutos)
# timestamp estÃ¡ em segundos, entÃ£o dividimos por 60
df_with_time_diff = df_with_prev \
    .withColumn("time_since_last_tx",
        when(col("prev_timestamp").isNotNull(),
            (col("timestamp") - col("prev_timestamp")) / 60  # Converte para minutos
        ).otherwise(None)  # Se nÃ£o tem transaÃ§Ã£o anterior, Ã© NULL
    )

# PASSO 4: Calcular distÃ¢ncia geogrÃ¡fica da transaÃ§Ã£o anterior
# Usamos PitÃ¡goras: distÃ¢ncia = âˆš((lat2-lat1)Â² + (lon2-lon1)Â²)
# NOTA: 1 grau de latitude â‰ˆ 111 km, entÃ£o multiplicamos por 111
df_with_distance = df_with_time_diff \
    .withColumn("distance_km_from_prev",
        when(col("prev_latitude").isNotNull(),
            sqrt(
                spark_pow(col("purchase_latitude") - col("prev_latitude"), 2) +
                spark_pow(col("purchase_longitude") - col("prev_longitude"), 2)
            ) * 111  # Converte graus para km (aproximado)
        ).otherwise(None)
    )

# ============================================================
# REGRA 2: VELOCIDADE IMPOSSÃVEL
# ============================================================
#
# CONCEITO: Se calculamos distÃ¢ncia e tempo entre duas compras,
# podemos calcular a VELOCIDADE necessÃ¡ria para se deslocar.
#
# FÃSICA BÃSICA:
#   velocidade = distÃ¢ncia / tempo
#   
# Se a velocidade for maior que um aviÃ£o (~900 km/h),
# Ã© FISICAMENTE IMPOSSÃVEL - forte indÃ­cio de fraude!
#
# Exemplo:
#   - Compra 1: SÃ£o Paulo Ã s 10:00
#   - Compra 2: Manaus Ã s 10:30 (30 minutos depois)
#   - DistÃ¢ncia: ~2.700 km
#   - Velocidade necessÃ¡ria: 2700 / 0.5 = 5.400 km/h 
#   - Isso Ã© 6x a velocidade de um aviÃ£o! IMPOSSÃVEL!
#
print("ğŸš€ Calculando velocidade entre transaÃ§Ãµes...")

df_with_velocity = df_with_distance \
    .withColumn("velocity_kmh",
        when(
            (col("distance_km_from_prev").isNotNull()) & 
            (col("time_since_last_tx") > 0),  # Evita divisÃ£o por zero
            # Velocidade = distÃ¢ncia(km) / tempo(horas)
            # time_since_last_tx estÃ¡ em minutos, entÃ£o dividimos por 60
            col("distance_km_from_prev") / (col("time_since_last_tx") / 60)
        ).otherwise(None)
    ) \
    .withColumn("is_impossible_velocity",
        # Se velocidade > 900 km/h (velocidade de aviÃ£o), Ã© impossÃ­vel!
        when(col("velocity_kmh") > 900, True).otherwise(False)
    )

# PASSO 5: Criar a FLAG de clonagem
# Suspeito se: tempo < 60 min E distÃ¢ncia > 555km (~5 graus) E estados diferentes
df_with_cloning = df_with_velocity \
    .withColumn("is_cloning_suspect",
        when(
            (col("time_since_last_tx") < 60) &                # Menos de 1 hora
            (col("distance_km_from_prev") > 555) &            # Mais de 555km de distÃ¢ncia
            (col("prev_state") != col("purchase_state"))      # Estados diferentes
        , True).otherwise(False)
    )

# TransformaÃ§Ãµes Silver - Limpeza e Enriquecimento
# THRESHOLDS REALISTAS para evitar excesso de flags
df_silver = df_with_cloning \
    .withColumn("amount_clean", 
        when(col("amount") < 0, spark_abs(col("amount")))
        .otherwise(col("amount"))) \
    .withColumn("distance_gps",
        spark_round(sqrt(
            spark_pow(col("device_latitude") - col("purchase_latitude"), 2) +
            spark_pow(col("device_longitude") - col("purchase_longitude"), 2)
        ), 4)) \
    .withColumn("is_cross_state",
        # Cross-state sÃ³ Ã© relevante SE nÃ£o houver histÃ³rico de viagem
        when((col("customer_home_state") != col("purchase_state")) & 
             (col("had_travel_purchase_last_12m") == False), True)
        .otherwise(False)) \
    .withColumn("is_night_transaction",
        # HorÃ¡rio suspeito: 2-5am 
        when((col("transaction_hour") >= 2) & (col("transaction_hour") < 5), True)
        .otherwise(False)) \
    .withColumn("is_high_value",
        # Valor alto: 5x a mÃ©dia 
        when(col("amount") > col("avg_transaction_amount_30d") * 5, True)
        .otherwise(False)) \
    .withColumn("is_high_velocity",
        # Alta velocidade: mais de 15 transaÃ§Ãµes em 24h (nÃ£o 5)
        when(col("transactions_last_24h") > 15, True)
        .otherwise(False)) \
    .withColumn("is_gps_mismatch",
        # GPS mismatch: distÃ¢ncia > 20 graus (~2222km) Ã© suspeito
        # Dados ShadowTraffic tÃªm mediana de 10 graus, p95=21 graus
        # Threshold alto para pegar apenas ~5% dos casos extremos
        when(col("distance_gps") > 20.0, True)
        .otherwise(False)) \
    .withColumn("is_risky_category",
        # REGRA 7: Categoria de Alto Risco
        # Categorias onde fraudes sÃ£o mais comuns: eletrÃ´nicos (alta revenda),
        # passagens aÃ©reas (alto valor, difÃ­cil cancelar)
        when(col("category").isin("electronics", "airline_ticket"), True)
        .otherwise(False)) \
    .withColumn("is_online_high_value",
        # REGRA 9: Compra Online de Alto Valor
        # Compras online > R$ 1000 sÃ£o mais arriscadas porque:
        # 1) NÃ£o precisa do cartÃ£o fÃ­sico
        # 2) EndereÃ§o de entrega pode ser diferente
        # 3) Mais fÃ¡cil para fraudadores
        when((col("is_online") == True) & (col("amount") > 1000), True)
        .otherwise(False)) \
    .withColumn("is_many_installments",
        # REGRA 10: Muitas Parcelas em Compra Grande
        # Fraudadores parcelam ao mÃ¡ximo para:
        # 1) Aumentar tempo antes da detecÃ§Ã£o
        # 2) Diluir o valor por fatura
        # 3) Dificultar rastreamento
        # 10+ parcelas com valor > R$ 500 Ã© suspeito
        when((col("installments") >= 10) & (col("amount") > 500), True)
        .otherwise(False)) \
    .withColumn("silver_timestamp", current_timestamp())

# Salvar Silver
silver_path = "s3a://fraud-data/medallion/silver/transactions"
print(f"ğŸ’¾ Salvando em: {silver_path}")

df_silver.write \
    .mode("overwrite") \
    .parquet(silver_path)

# Verificar
df_check = spark.read.parquet(silver_path)
print(f"âœ… SILVER CONCLUÃDO: {df_check.count():,} registros limpos")

# Mostrar estatÃ­sticas
print("\nğŸ“Š EstatÃ­sticas Silver:")
print(f"   - TransaÃ§Ãµes cross-state: {df_check.filter(col('is_cross_state')).count():,}")
print(f"   - TransaÃ§Ãµes noturnas: {df_check.filter(col('is_night_transaction')).count():,}")
print(f"   - Alto valor: {df_check.filter(col('is_high_value')).count():,}")
print(f"   - GPS suspeito: {df_check.filter(col('is_gps_mismatch')).count():,}")
print(f"   - Categoria de risco: {df_check.filter(col('is_risky_category')).count():,}")
print(f"   - Online alto valor: {df_check.filter(col('is_online_high_value')).count():,}")
print(f"   - Muitas parcelas: {df_check.filter(col('is_many_installments')).count():,}")
print(f"   - ğŸš¨ Suspeita clonagem: {df_check.filter(col('is_cloning_suspect')).count():,}")
print(f"   - ğŸš€ Velocidade impossÃ­vel: {df_check.filter(col('is_impossible_velocity')).count():,}")
print("=" * 60)

spark.stop()
