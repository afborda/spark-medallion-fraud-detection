"""
üì¶ LOAD TO POSTGRES - Gold ‚Üí PostgreSQL
Carrega dados da camada Gold para PostgreSQL para BI/Metabase
"""

import sys
sys.path.insert(0, '/jobs')

from pyspark.sql import SparkSession
from pyspark.sql.functions import col
from config import (
    MINIO_ENDPOINT, MINIO_ACCESS_KEY, MINIO_SECRET_KEY,
    POSTGRES_URL, POSTGRES_PROPERTIES, GOLD_PATH
)

print("=" * 60)
print("üì¶ LOAD TO POSTGRES - Gold ‚Üí PostgreSQL")
print("üáßüá∑ Dados brasileiros")
print("=" * 60)

spark = SparkSession.builder \
    .appName("Load_to_Postgres") \
    .config("spark.hadoop.fs.s3a.endpoint", MINIO_ENDPOINT) \
    .config("spark.hadoop.fs.s3a.access.key", MINIO_ACCESS_KEY) \
    .config("spark.hadoop.fs.s3a.secret.key", MINIO_SECRET_KEY) \
    .config("spark.hadoop.fs.s3a.path.style.access", "true") \
    .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
    .config("spark.hadoop.fs.s3a.connection.ssl.enabled", "false") \
    .getOrCreate()

spark.sparkContext.setLogLevel("WARN")

GOLD_BASE = f"{GOLD_PATH}"

# ============================================
# 1. CARREGAR TRANSACTIONS (fraud_detection)
# ============================================
print("\n" + "=" * 40)
print("üí≥ Carregando TRANSACTIONS...")
print("=" * 40)

df_transactions = spark.read.parquet(f"{GOLD_BASE}/fraud_detection")

# Selecionar campos para PostgreSQL (evitar campos muito grandes)
df_tx_pg = df_transactions.select(
    "transaction_id",
    "customer_id",
    "timestamp_dt",
    "tx_date",
    "tx_year",
    "tx_month",
    "tx_hour",
    "tipo",
    col("valor").alias("amount"),
    "canal",
    "merchant_name",
    "merchant_category",
    "mcc_code",
    "mcc_risk_level",
    "bandeira",
    "entrada_cartao",
    "status",
    "motivo_recusa",
    "fraud_score",
    "fraud_score_category",
    "is_fraud",
    "fraud_type",
    "risk_points",
    "risk_level",
    "requires_review",
    "periodo_dia",
    "faixa_valor",
    "is_weekend",
    "_gold_timestamp"
)

tx_count = df_tx_pg.count()
print(f"‚úÖ {tx_count:,} transa√ß√µes para carregar")

df_tx_pg.write \
    .mode("overwrite") \
    .jdbc(POSTGRES_URL, "transactions", properties=POSTGRES_PROPERTIES)

print(f"üíæ Tabela 'transactions' criada no PostgreSQL")

# ============================================
# 2. CARREGAR FRAUD_ALERTS
# ============================================
print("\n" + "=" * 40)
print("‚ö†Ô∏è Carregando FRAUD_ALERTS...")
print("=" * 40)

df_alerts = spark.read.parquet(f"{GOLD_BASE}/fraud_alerts")

alert_count = df_alerts.count()
print(f"‚úÖ {alert_count:,} alertas para carregar")

df_alerts.write \
    .mode("overwrite") \
    .jdbc(POSTGRES_URL, "fraud_alerts", properties=POSTGRES_PROPERTIES)

print(f"üíæ Tabela 'fraud_alerts' criada no PostgreSQL")

# ============================================
# 3. CARREGAR CUSTOMER_SUMMARY
# ============================================
print("\n" + "=" * 40)
print("üë§ Carregando CUSTOMER_SUMMARY...")
print("=" * 40)

df_customers = spark.read.parquet(f"{GOLD_BASE}/customer_summary")

customer_count = df_customers.count()
print(f"‚úÖ {customer_count:,} clientes para carregar")

df_customers.write \
    .mode("overwrite") \
    .jdbc(POSTGRES_URL, "customer_summary", properties=POSTGRES_PROPERTIES)

print(f"üíæ Tabela 'customer_summary' criada no PostgreSQL")

# ============================================
# 4. CARREGAR FRAUD_METRICS
# ============================================
print("\n" + "=" * 40)
print("üìà Carregando FRAUD_METRICS...")
print("=" * 40)

df_metrics = spark.read.parquet(f"{GOLD_BASE}/fraud_metrics")

metrics_count = df_metrics.count()
print(f"‚úÖ {metrics_count} m√©tricas para carregar")

df_metrics.write \
    .mode("overwrite") \
    .jdbc(POSTGRES_URL, "fraud_metrics", properties=POSTGRES_PROPERTIES)

print(f"üíæ Tabela 'fraud_metrics' criada no PostgreSQL")

# ============================================
# SUM√ÅRIO FINAL
# ============================================
print("\n" + "=" * 60)
print("‚úÖ LOAD TO POSTGRES CONCLU√çDO!")
print("=" * 60)
print(f"üìä Tabelas criadas no PostgreSQL:")
print(f"   üí≥ transactions: {tx_count:,} registros")
print(f"   ‚ö†Ô∏è fraud_alerts: {alert_count:,} registros")
print(f"   üë§ customer_summary: {customer_count:,} registros")
print(f"   üìà fraud_metrics: {metrics_count} registros")
print("=" * 60)

spark.stop()
