# ğŸš€ Melhorias Futuras - Pipeline de DetecÃ§Ã£o de Fraudes

> **Documento de EvoluÃ§Ã£o do Projeto**  
> Este documento descreve melhorias planejadas para tornar o projeto ainda mais profissional e production-ready.

---

## ğŸ“Š Status Atual do Projeto

### âœ… JÃ¡ Implementado
| Componente | Tecnologia | Status |
|------------|------------|--------|
| OrquestraÃ§Ã£o | Docker Compose | âœ… Manual |
| Data Lake | MinIO (S3-compatible) | âœ… Funcionando |
| Processamento Batch | Apache Spark 3.5.3 | âœ… 51GB processados |
| Processamento Streaming | Spark Structured Streaming | âœ… Funcionando |
| Mensageria | Apache Kafka | âœ… 5M+ mensagens |
| Banco de Dados | PostgreSQL 16 | âœ… 48M transaÃ§Ãµes |
| VisualizaÃ§Ã£o | Metabase | âœ… Dashboards pÃºblicos |
| GeraÃ§Ã£o de Dados | ShadowTraffic | âœ… Real-time |

---

## ğŸ¯ Melhorias PrioritÃ¡rias

### 1. ğŸ”„ Apache Airflow - OrquestraÃ§Ã£o de Pipelines

**Por que adicionar?**
- Agendamento automÃ¡tico de jobs
- Monitoramento visual de DAGs
- Retry automÃ¡tico em falhas
- Alertas por email/Slack
- HistÃ³rico de execuÃ§Ãµes

**Arquitetura Proposta:**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    APACHE AIRFLOW                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  DAG: fraud_detection_daily                                 â”‚
â”‚  â”œâ”€â”€ Task 1: check_new_data (Sensor)                       â”‚
â”‚  â”œâ”€â”€ Task 2: bronze_ingestion (SparkSubmitOperator)        â”‚
â”‚  â”œâ”€â”€ Task 3: silver_transformation (SparkSubmitOperator)   â”‚
â”‚  â”œâ”€â”€ Task 4: gold_aggregation (SparkSubmitOperator)        â”‚
â”‚  â”œâ”€â”€ Task 5: export_to_postgres (SparkSubmitOperator)      â”‚
â”‚  â””â”€â”€ Task 6: notify_completion (SlackOperator)             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ConfiguraÃ§Ã£o Docker:**
```yaml
# Adicionar ao docker-compose.yml
airflow-webserver:
  image: apache/airflow:2.8.1-python3.11
  environment:
    - AIRFLOW__CORE__EXECUTOR=LocalExecutor
    - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@airflow-postgres:5432/airflow
    - AIRFLOW__WEBSERVER__SECRET_KEY=your-secret-key
    - AIRFLOW__CORE__FERNET_KEY=your-fernet-key
  volumes:
    - ./airflow/dags:/opt/airflow/dags
    - ./airflow/logs:/opt/airflow/logs
    - ./airflow/plugins:/opt/airflow/plugins
  ports:
    - "8080:8080"
  depends_on:
    - airflow-postgres
  command: webserver

airflow-scheduler:
  image: apache/airflow:2.8.1-python3.11
  environment:
    - AIRFLOW__CORE__EXECUTOR=LocalExecutor
    - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@airflow-postgres:5432/airflow
  volumes:
    - ./airflow/dags:/opt/airflow/dags
    - ./airflow/logs:/opt/airflow/logs
  depends_on:
    - airflow-webserver
  command: scheduler

airflow-postgres:
  image: postgres:16
  environment:
    - POSTGRES_USER=airflow
    - POSTGRES_PASSWORD=airflow
    - POSTGRES_DB=airflow
  volumes:
    - ./docker_volumes/airflow_postgres:/var/lib/postgresql/data
```

**Exemplo de DAG:**
```python
# airflow/dags/fraud_detection_dag.py
from airflow import DAG
from airflow.providers.apache.spark.operators.spark_submit import SparkSubmitOperator
from airflow.operators.python import PythonOperator
from airflow.sensors.filesystem import FileSensor
from datetime import datetime, timedelta

default_args = {
    'owner': 'data_engineering',
    'depends_on_past': False,
    'start_date': datetime(2024, 1, 1),
    'email_on_failure': True,
    'email': ['alerts@company.com'],
    'retries': 3,
    'retry_delay': timedelta(minutes=5),
}

with DAG(
    'fraud_detection_pipeline',
    default_args=default_args,
    description='Pipeline de DetecÃ§Ã£o de Fraudes - Medallion Architecture',
    schedule_interval='0 */6 * * *',  # A cada 6 horas
    catchup=False,
    tags=['fraud', 'spark', 'medallion'],
) as dag:

    bronze_task = SparkSubmitOperator(
        task_id='bronze_ingestion',
        application='/opt/spark/jobs/batch/bronze_ingestion.py',
        conn_id='spark_default',
        conf={
            'spark.executor.memory': '4g',
            'spark.driver.memory': '2g',
        },
    )

    silver_task = SparkSubmitOperator(
        task_id='silver_transformation',
        application='/opt/spark/jobs/batch/silver_transformation.py',
        conn_id='spark_default',
    )

    gold_task = SparkSubmitOperator(
        task_id='gold_aggregation',
        application='/opt/spark/jobs/batch/gold_aggregation.py',
        conn_id='spark_default',
    )

    bronze_task >> silver_task >> gold_task
```

---

### 2. ğŸ“ˆ Apache Superset - BI AvanÃ§ado

**Por que adicionar?**
- VisualizaÃ§Ãµes mais avanÃ§adas que Metabase
- Suporte nativo a SQL Lab
- Dashboards interativos com drill-down
- Alertas e relatÃ³rios automatizados
- Melhor para grandes volumes de dados

**ConfiguraÃ§Ã£o Docker:**
```yaml
superset:
  image: apache/superset:3.1.0
  environment:
    - SUPERSET_SECRET_KEY=your-secret-key
    - DATABASE_URL=postgresql://superset:superset@superset-postgres:5432/superset
  ports:
    - "8088:8088"
  volumes:
    - ./docker_volumes/superset:/app/superset_home
  depends_on:
    - superset-postgres
```

---

### 3. ğŸ” Great Expectations - Data Quality

**Por que adicionar?**
- ValidaÃ§Ã£o automÃ¡tica de qualidade de dados
- DocumentaÃ§Ã£o de expectativas
- Alertas quando dados nÃ£o conformes
- IntegraÃ§Ã£o com Airflow

**Exemplo de Expectativas:**
```python
# great_expectations/expectations/transactions_suite.py
import great_expectations as gx

context = gx.get_context()

suite = context.add_expectation_suite("transactions_suite")

# Expectativas para transaÃ§Ãµes
suite.add_expectation(
    gx.expectations.ExpectColumnValuesToBeUnique(column="transaction_id")
)
suite.add_expectation(
    gx.expectations.ExpectColumnValuesToBeBetween(
        column="amount", min_value=0.01, max_value=1000000
    )
)
suite.add_expectation(
    gx.expectations.ExpectColumnValuesToNotBeNull(column="customer_id")
)
suite.add_expectation(
    gx.expectations.ExpectColumnValuesToBeInSet(
        column="payment_method",
        value_set=["credit_card", "debit_card", "pix", "boleto"]
    )
)
```

---

### 4. ğŸ›¡ï¸ Vault - Gerenciamento de Secrets

**Por que adicionar?**
- CentralizaÃ§Ã£o de credenciais
- RotaÃ§Ã£o automÃ¡tica de senhas
- Auditoria de acesso
- EliminaÃ§Ã£o de hardcoded credentials

**ConfiguraÃ§Ã£o Docker:**
```yaml
vault:
  image: hashicorp/vault:1.15
  cap_add:
    - IPC_LOCK
  environment:
    - VAULT_DEV_ROOT_TOKEN_ID=myroot
    - VAULT_DEV_LISTEN_ADDRESS=0.0.0.0:8200
  ports:
    - "8200:8200"
  volumes:
    - ./docker_volumes/vault:/vault/data
```

---

### 5. ğŸ“Š Prometheus + Grafana - Observabilidade

**Por que adicionar?**
- MÃ©tricas de infraestrutura em tempo real
- Alertas configurÃ¡veis
- Dashboards de performance
- SLAs e SLOs

**ConfiguraÃ§Ã£o Docker:**
```yaml
prometheus:
  image: prom/prometheus:v2.48.0
  ports:
    - "9090:9090"
  volumes:
    - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
    - ./docker_volumes/prometheus:/prometheus
  command:
    - '--config.file=/etc/prometheus/prometheus.yml'
    - '--storage.tsdb.path=/prometheus'

grafana:
  image: grafana/grafana:10.2.3
  ports:
    - "3000:3000"
  environment:
    - GF_SECURITY_ADMIN_PASSWORD=admin
    - GF_USERS_ALLOW_SIGN_UP=false
  volumes:
    - ./docker_volumes/grafana:/var/lib/grafana
    - ./monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards
  depends_on:
    - prometheus
```

**Exemplo prometheus.yml:**
```yaml
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'spark'
    static_configs:
      - targets: ['spark-master:4040', 'spark-master:8080']
  
  - job_name: 'kafka'
    static_configs:
      - targets: ['kafka:9092']
  
  - job_name: 'postgres'
    static_configs:
      - targets: ['postgres-exporter:9187']
```

---

### 6. ğŸ”„ dbt (Data Build Tool) - TransformaÃ§Ãµes SQL

**Por que adicionar?**
- TransformaÃ§Ãµes versionadas
- Testes automatizados
- DocumentaÃ§Ã£o gerada automaticamente
- Lineage de dados

**Estrutura de Projeto dbt:**
```
dbt/
â”œâ”€â”€ dbt_project.yml
â”œâ”€â”€ profiles.yml
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ staging/
â”‚   â”‚   â”œâ”€â”€ stg_transactions.sql
â”‚   â”‚   â””â”€â”€ stg_customers.sql
â”‚   â”œâ”€â”€ intermediate/
â”‚   â”‚   â””â”€â”€ int_transaction_enriched.sql
â”‚   â””â”€â”€ marts/
â”‚       â”œâ”€â”€ fct_fraud_alerts.sql
â”‚       â””â”€â”€ dim_customers.sql
â”œâ”€â”€ tests/
â”‚   â””â”€â”€ assert_positive_amounts.sql
â””â”€â”€ macros/
    â””â”€â”€ fraud_rules.sql
```

**Exemplo de Modelo dbt:**
```sql
-- models/marts/fct_fraud_alerts.sql
{{ config(materialized='incremental', unique_key='alert_id') }}

WITH transactions AS (
    SELECT * FROM {{ ref('stg_transactions') }}
    {% if is_incremental() %}
    WHERE transaction_date > (SELECT MAX(transaction_date) FROM {{ this }})
    {% endif %}
),

fraud_detection AS (
    SELECT
        transaction_id,
        customer_id,
        amount,
        CASE
            WHEN amount > 10000 THEN 'HIGH_VALUE'
            WHEN velocity_score > 5 THEN 'HIGH_VELOCITY'
            WHEN distance_km > 500 AND time_diff_minutes < 60 THEN 'IMPOSSIBLE_TRAVEL'
            ELSE NULL
        END AS fraud_type
    FROM transactions
)

SELECT * FROM fraud_detection WHERE fraud_type IS NOT NULL
```

---

### 7. ğŸ§ª MLflow - Machine Learning Ops

**Por que adicionar?**
- Versionamento de modelos
- Tracking de experimentos
- Deploy de modelos
- A/B testing

**ConfiguraÃ§Ã£o Docker:**
```yaml
mlflow:
  image: ghcr.io/mlflow/mlflow:v2.10.0
  ports:
    - "5000:5000"
  environment:
    - MLFLOW_BACKEND_STORE_URI=postgresql://mlflow:mlflow@mlflow-postgres:5432/mlflow
    - MLFLOW_DEFAULT_ARTIFACT_ROOT=s3://mlflow-artifacts/
    - AWS_ACCESS_KEY_ID=minioadmin
    - AWS_SECRET_ACCESS_KEY=minioadmin123@@!!_2
    - MLFLOW_S3_ENDPOINT_URL=http://minio:9000
  volumes:
    - ./docker_volumes/mlflow:/mlflow
  command: mlflow server --host 0.0.0.0 --port 5000
```

**Exemplo de Treinamento:**
```python
# ml/train_fraud_model.py
import mlflow
import mlflow.sklearn
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_score, recall_score, f1_score

mlflow.set_tracking_uri("http://mlflow:5000")
mlflow.set_experiment("fraud_detection")

with mlflow.start_run(run_name="random_forest_v1"):
    # ParÃ¢metros
    params = {
        "n_estimators": 100,
        "max_depth": 10,
        "min_samples_split": 5,
    }
    mlflow.log_params(params)
    
    # Treinamento
    model = RandomForestClassifier(**params)
    model.fit(X_train, y_train)
    
    # MÃ©tricas
    y_pred = model.predict(X_test)
    mlflow.log_metrics({
        "precision": precision_score(y_test, y_pred),
        "recall": recall_score(y_test, y_pred),
        "f1": f1_score(y_test, y_pred),
    })
    
    # Salvar modelo
    mlflow.sklearn.log_model(model, "fraud_model")
```

---

### 8. ğŸ“ Elasticsearch + Kibana - Logs Centralizados

**Por que adicionar?**
- Logs centralizados de todos os serviÃ§os
- Busca full-text em logs
- Dashboards de anÃ¡lise de logs
- Alertas baseados em padrÃµes

**ConfiguraÃ§Ã£o Docker:**
```yaml
elasticsearch:
  image: docker.elastic.co/elasticsearch/elasticsearch:8.11.3
  environment:
    - discovery.type=single-node
    - xpack.security.enabled=false
    - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
  ports:
    - "9200:9200"
  volumes:
    - ./docker_volumes/elasticsearch:/usr/share/elasticsearch/data

kibana:
  image: docker.elastic.co/kibana/kibana:8.11.3
  ports:
    - "5601:5601"
  environment:
    - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
  depends_on:
    - elasticsearch

filebeat:
  image: docker.elastic.co/beats/filebeat:8.11.3
  volumes:
    - ./monitoring/filebeat.yml:/usr/share/filebeat/filebeat.yml
    - /var/lib/docker/containers:/var/lib/docker/containers:ro
    - /var/run/docker.sock:/var/run/docker.sock:ro
  depends_on:
    - elasticsearch
```

---

## ğŸ—ï¸ Arquitetura Completa Proposta

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              ARQUITETURA PRODUCTION-READY                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”              â”‚
â”‚  â”‚ ShadowTrafficâ”‚    â”‚   Airflow   â”‚    â”‚    Vault    â”‚    â”‚   MLflow    â”‚              â”‚
â”‚  â”‚  (GeraÃ§Ã£o)  â”‚    â”‚(OrquestraÃ§Ã£o)â”‚    â”‚  (Secrets)  â”‚    â”‚    (ML)     â”‚              â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜              â”‚
â”‚         â”‚                  â”‚                  â”‚                  â”‚                      â”‚
â”‚         â–¼                  â–¼                  â–¼                  â–¼                      â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚                          APACHE KAFKA                                â”‚               â”‚
â”‚  â”‚                     (Mensageria Central)                             â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                                  â”‚                                                      â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”‚
â”‚         â”‚                        â”‚                        â”‚                            â”‚
â”‚         â–¼                        â–¼                        â–¼                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚  â”‚    Spark    â”‚          â”‚    Spark    â”‚          â”‚Great Expect.â”‚                     â”‚
â”‚  â”‚   Batch     â”‚          â”‚  Streaming  â”‚          â”‚(Data Quality)â”‚                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚         â”‚                        â”‚                        â”‚                            â”‚
â”‚         â–¼                        â–¼                        â–¼                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚                    MinIO DATA LAKE (S3)                              â”‚               â”‚
â”‚  â”‚        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚               â”‚
â”‚  â”‚        â”‚  Bronze  â”‚â†’ â”‚  Silver  â”‚â†’ â”‚   Gold   â”‚                     â”‚               â”‚
â”‚  â”‚        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                                  â”‚                                                      â”‚
â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”‚
â”‚         â”‚                        â”‚                        â”‚                            â”‚
â”‚         â–¼                        â–¼                        â–¼                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                     â”‚
â”‚  â”‚ PostgreSQL  â”‚          â”‚    dbt      â”‚          â”‚  Superset   â”‚                     â”‚
â”‚  â”‚   (OLTP)    â”‚          â”‚(Transform)  â”‚          â”‚(VisualizaÃ§Ã£o)â”‚                    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                     â”‚
â”‚                                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”               â”‚
â”‚  â”‚                      OBSERVABILIDADE                                 â”‚               â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”            â”‚               â”‚
â”‚  â”‚  â”‚Prometheusâ”‚  â”‚ Grafana  â”‚  â”‚  Kibana  â”‚  â”‚Elasticsearchâ”‚          â”‚               â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â”‚               â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜               â”‚
â”‚                                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‹ Roadmap de ImplementaÃ§Ã£o

### Fase 1: OrquestraÃ§Ã£o (1-2 semanas)
| Tarefa | Prioridade | Complexidade |
|--------|------------|--------------|
| Instalar Apache Airflow | ğŸ”´ Alta | MÃ©dia |
| Criar DAGs para batch pipeline | ğŸ”´ Alta | MÃ©dia |
| Configurar alertas de falha | ğŸŸ¡ MÃ©dia | Baixa |
| Documentar DAGs | ğŸŸ¡ MÃ©dia | Baixa |

### Fase 2: Qualidade de Dados (1 semana)
| Tarefa | Prioridade | Complexidade |
|--------|------------|--------------|
| Instalar Great Expectations | ğŸŸ¡ MÃ©dia | MÃ©dia |
| Criar expectation suites | ğŸŸ¡ MÃ©dia | MÃ©dia |
| Integrar com Airflow | ğŸŸ¡ MÃ©dia | Baixa |

### Fase 3: Observabilidade (1-2 semanas)
| Tarefa | Prioridade | Complexidade |
|--------|------------|--------------|
| Instalar Prometheus + Grafana | ğŸŸ¡ MÃ©dia | MÃ©dia |
| Configurar exporters | ğŸŸ¡ MÃ©dia | MÃ©dia |
| Criar dashboards de infra | ğŸŸ¡ MÃ©dia | MÃ©dia |
| Instalar ELK Stack | ğŸŸ¢ Baixa | Alta |

### Fase 4: Machine Learning (2-3 semanas)
| Tarefa | Prioridade | Complexidade |
|--------|------------|--------------|
| Instalar MLflow | ğŸŸ¡ MÃ©dia | MÃ©dia |
| Treinar modelo de fraude | ğŸŸ¡ MÃ©dia | Alta |
| Deploy do modelo | ğŸŸ¡ MÃ©dia | Alta |
| Integrar com pipeline | ğŸŸ¡ MÃ©dia | MÃ©dia |

### Fase 5: SeguranÃ§a (1 semana)
| Tarefa | Prioridade | Complexidade |
|--------|------------|--------------|
| Instalar Vault | ğŸŸ¢ Baixa | MÃ©dia |
| Migrar secrets | ğŸŸ¢ Baixa | MÃ©dia |
| Configurar rotaÃ§Ã£o | ğŸŸ¢ Baixa | Baixa |

---

## ğŸ’° Estimativa de Recursos

### Recursos Atuais
```
Total: ~16GB RAM, 8 vCPUs
- Spark Master: 2GB
- Spark Workers (2x): 4GB cada
- Kafka: 2GB
- PostgreSQL: 1GB
- MinIO: 1GB
- Metabase: 1GB
- Outros: 1GB
```

### Recursos com Todas Melhorias
```
Total: ~32GB RAM, 16 vCPUs (recomendado)
+ Airflow: 2GB
+ Prometheus/Grafana: 2GB
+ Elasticsearch/Kibana: 4GB
+ MLflow: 2GB
+ Vault: 512MB
+ Superset: 2GB
+ dbt: 512MB
```

---

## ğŸ¯ Quick Wins (ImplementaÃ§Ã£o RÃ¡pida)

### 1. Adicionar Health Checks no Docker Compose
```yaml
services:
  spark-master:
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
      interval: 30s
      timeout: 10s
      retries: 3
```

### 2. Adicionar Labels para OrganizaÃ§Ã£o
```yaml
services:
  spark-master:
    labels:
      - "com.fraud.service=spark"
      - "com.fraud.layer=processing"
      - "com.fraud.tier=core"
```

### 3. Adicionar Limites de Recursos
```yaml
services:
  spark-master:
    deploy:
      resources:
        limits:
          cpus: '2'
          memory: 4G
        reservations:
          memory: 2G
```

### 4. Adicionar Rede Dedicada
```yaml
networks:
  fraud-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16
```

---

## ğŸ“š Recursos de Aprendizado

### Airflow
- [DocumentaÃ§Ã£o Oficial](https://airflow.apache.org/docs/)
- [Astronomer Guides](https://www.astronomer.io/guides/)
- [Curso Gratuito - Udemy](https://www.udemy.com/course/the-complete-hands-on-course-to-master-apache-airflow/)

### MLflow
- [MLflow Docs](https://mlflow.org/docs/latest/index.html)
- [ML Engineering for Production](https://www.coursera.org/specializations/machine-learning-engineering-for-production-mlops)

### Great Expectations
- [Getting Started](https://docs.greatexpectations.io/docs/tutorials/getting_started/tutorial_overview)
- [IntegraÃ§Ã£o com Airflow](https://docs.greatexpectations.io/docs/deployment_patterns/how_to_use_great_expectations_in_airflow)

### Observabilidade
- [Prometheus + Grafana](https://prometheus.io/docs/visualization/grafana/)
- [ELK Stack](https://www.elastic.co/guide/index.html)

---

## âœ… Checklist de ImplementaÃ§Ã£o

```
[ ] Apache Airflow
    [ ] InstalaÃ§Ã£o e configuraÃ§Ã£o
    [ ] DAG batch pipeline
    [ ] DAG streaming pipeline
    [ ] Alertas configurados
    [ ] DocumentaÃ§Ã£o

[ ] Great Expectations
    [ ] InstalaÃ§Ã£o
    [ ] Transaction expectations
    [ ] Customer expectations
    [ ] IntegraÃ§Ã£o Airflow

[ ] Prometheus + Grafana
    [ ] InstalaÃ§Ã£o
    [ ] Exporters configurados
    [ ] Dashboards criados
    [ ] Alertas configurados

[ ] MLflow
    [ ] InstalaÃ§Ã£o
    [ ] Modelo treinado
    [ ] Deploy em produÃ§Ã£o
    [ ] IntegraÃ§Ã£o pipeline

[ ] Vault
    [ ] InstalaÃ§Ã£o
    [ ] Secrets migrados
    [ ] RotaÃ§Ã£o configurada

[ ] ELK Stack (Opcional)
    [ ] InstalaÃ§Ã£o
    [ ] Filebeat configurado
    [ ] Dashboards Kibana
```

---

## ğŸ“ ConclusÃ£o

Este documento serve como guia para evoluÃ§Ã£o do projeto. As melhorias foram priorizadas considerando:

1. **Impacto no portfÃ³lio** - Demonstra conhecimento de ferramentas enterprise
2. **Complexidade** - Balanceando tempo de implementaÃ§Ã£o vs benefÃ­cio
3. **Recursos** - Considerando limitaÃ§Ãµes de hardware

**RecomendaÃ§Ã£o**: ComeÃ§ar pelo **Apache Airflow**, pois Ã© a melhoria com maior visibilidade e demonstra habilidades de orquestraÃ§Ã£o de pipelines, muito demandada no mercado.

---

> **Autor**: Abner Fonseca  
> **Data**: Dezembro 2024  
> **VersÃ£o**: 1.0
