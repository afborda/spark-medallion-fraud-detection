# üè¶ Bank Fraud Detection - Data Pipeline
## Documenta√ß√£o Completa para Estudo e Reprodu√ß√£o

> **√öltima atualiza√ß√£o:** 2025-12-02
> **Status atual:** 51GB de dados brasileiros processados com sucesso! üáßüá∑

---

## üìä M√âTRICAS DO PIPELINE ATUAL

| M√©trica | Valor |
|---------|-------|
| **Transa√ß√µes Raw** | 51,281,996 |
| **Transa√ß√µes Processadas** | 48,445,853 (5.5% removidas na limpeza) |
| **Dados Raw (JSON)** | 51 GB (479 arquivos) |
| **Bronze (Parquet)** | 5.0 GB |
| **Silver (Parquet)** | 5.4 GB |
| **Gold (Parquet)** | 2.0 GB |
| **Total MinIO** | 12 GB |
| **Clientes** | 100,000 (nomes brasileiros) |
| **Dispositivos** | 300,102 |
| **Tempo Total Pipeline** | ~34 min |
| **Compress√£o** | 90% (51GB ‚Üí 5GB) |

---

# üìö √çNDICE

1. [Vis√£o Geral do Projeto](#1-vis√£o-geral-do-projeto)
2. [Arquitetura Medallion](#2-arquitetura-medallion)
3. [Stack Tecnol√≥gica](#3-stack-tecnol√≥gica)
4. [Infraestrutura Docker](#4-infraestrutura-docker)
5. [Configura√ß√£o do Kafka](#5-configura√ß√£o-do-kafka)
6. [ShadowTraffic - Gerador de Dados](#6-shadowtraffic---gerador-de-dados)
7. [Spark - Processamento de Dados](#7-spark---processamento-de-dados)
8. [MinIO - Data Lake](#8-minio---data-lake)
9. [PostgreSQL - Banco Anal√≠tico](#9-postgresql---banco-anal√≠tico)
10. [Fluxo Completo de Dados](#10-fluxo-completo-de-dados)
11. [Problemas Encontrados e Solu√ß√µes](#11-problemas-encontrados-e-solu√ß√µes)
12. [Comandos √öteis](#12-comandos-√∫teis)
13. [Como Reproduzir do Zero](#13-como-reproduzir-do-zero)

---

# 1. VIS√ÉO GERAL DO PROJETO

## O que √© este projeto?
Um pipeline de dados para **detec√ß√£o de fraudes em transa√ß√µes banc√°rias** em tempo real/batch.

## Objetivo
Simular um cen√°rio real de uma empresa que precisa:
1. Receber milhares de transa√ß√µes por segundo
2. Processar e enriquecer esses dados
3. Aplicar regras de detec√ß√£o de fraude
4. Armazenar em camadas organizadas (Bronze ‚Üí Silver ‚Üí Gold)
5. Disponibilizar para dashboards e an√°lises

## Fluxo Resumido
```
ShadowTraffic ‚Üí Kafka ‚Üí Spark ‚Üí MinIO (Data Lake) ‚Üí PostgreSQL ‚Üí Dashboard
    (gera)      (fila)  (processa) (armazena)        (anal√≠tico)   (visualiza)
```

---

# 2. ARQUITETURA MEDALLION

## O que √©?
Padr√£o de organiza√ß√£o de dados em 3 camadas, usado por empresas como Databricks, Netflix, etc.

## As 3 Camadas

### ü•â BRONZE (Raw/Crua)
- **O que √©**: Dados brutos, exatamente como chegaram
- **Formato**: JSON original do Kafka
- **Transforma√ß√µes**: Nenhuma (apenas adiciona metadados como timestamp de ingest√£o)
- **Uso**: Auditoria, reprocessamento, debugging

### ü•à SILVER (Cleaned/Limpa)
- **O que √©**: Dados limpos e padronizados
- **Transforma√ß√µes aplicadas**:
  - Convers√£o de tipos (string ‚Üí double, etc)
  - Tratamento de nulos
  - Remo√ß√£o de duplicatas
  - Valida√ß√£o de schema
  - C√°lculos derivados (dist√¢ncia GPS, flags de risco)
- **Uso**: Fonte para an√°lises e modelos de ML

### ü•á GOLD (Business/Neg√≥cio)
- **O que √©**: Dados agregados e prontos para consumo
- **Transforma√ß√µes aplicadas**:
  - C√°lculo do Fraud Score
  - Classifica√ß√£o de risco (CR√çTICO, ALTO, M√âDIO, BAIXO, NORMAL)
  - Agrega√ß√µes por categoria, per√≠odo, etc
- **Uso**: Dashboards, relat√≥rios, alertas

---

# 3. STACK TECNOL√ìGICA

## Componentes e suas fun√ß√µes

| Tecnologia | Fun√ß√£o | Por que usar? |
|------------|--------|---------------|
| **Apache Kafka** | Message Broker | Fila de mensagens distribu√≠da, alta throughput |
| **Apache Spark** | Processamento | Engine de Big Data, batch e streaming |
| **MinIO** | Object Storage | Data Lake compat√≠vel com S3 |
| **PostgreSQL** | Banco Relacional | Armazenamento anal√≠tico para dashboards |
| **Metabase** | BI Dashboard | Visualiza√ß√£o de dados (porta 3000) |
| **Faker pt_BR** | Gerador de Dados | Dados brasileiros realistas |
| **Docker** | Containeriza√ß√£o | Ambiente isolado e reproduz√≠vel |

## Vers√µes Importantes
```
Spark: 3.5.3 (N√ÉO usar 4.x - tem bug com MinIO)
Kafka: 3.5.1
PostgreSQL: 16
MinIO: Latest
Metabase: Latest
Python: 3.13
```

---

# 4. INFRAESTRUTURA DOCKER

## Arquivo: docker-compose.yml

### Containers criados:
1. **fraud_kafka** - Broker de mensagens (porta 9092)
2. **fraud_spark_master** - Coordenador Spark (porta 8081)
3. **fraud_spark_worker_1 a 5** - Workers Spark (10 cores, 15GB RAM total)
4. **fraud_minio** - Object Storage (porta 9002 API, 9003 console)
5. **fraud_postgres** - Banco de dados (porta 5432)
6. **fraud_metabase** - Dashboard BI (porta 3000)

### Network
Todos os containers est√£o na mesma rede Docker:
```
1_projeto_bank_fraud_detection_data_pipeline_default
```

### Volumes mapeados
```yaml
# Jobs Spark
./spark/jobs:/jobs

# JARs necess√°rios
./jars:/jars

# Dados
./data:/data
```

### Como subir a infraestrutura
```bash
cd /home/ubuntu/Estudos/1_projeto_bank_Fraud_detection_data_pipeline
docker-compose up -d
```

### Como verificar se est√° tudo rodando
```bash
docker ps --format "table {{.Names}}\t{{.Status}}"
```

---

# 5. CONFIGURA√á√ÉO DO KAFKA

## O que √© Kafka?
Sistema de mensageria distribu√≠do. Funciona como uma "fila inteligente" onde:
- **Producers** enviam mensagens para **Topics**
- **Consumers** leem mensagens dos **Topics**
- Mensagens s√£o persistidas e podem ser relidas

## Conceitos importantes

### Topics
"Categorias" de mensagens. Criamos 2:
- `transactions` - Transa√ß√µes banc√°rias (5 parti√ß√µes)
- `customers` - Dados de clientes (3 parti√ß√µes)

### Parti√ß√µes
Divis√µes dentro de um topic para paralelismo. Mais parti√ß√µes = mais throughput.

### Offsets
Posi√ß√£o de leitura no topic:
- `earliest` - L√™ desde o in√≠cio
- `latest` - L√™ apenas novos dados

## Comandos Kafka

### Criar topics
```bash
docker exec fraud_kafka kafka-topics --create \
    --topic transactions \
    --bootstrap-server localhost:9092 \
    --partitions 5 \
    --replication-factor 1

docker exec fraud_kafka kafka-topics --create \
    --topic customers \
    --bootstrap-server localhost:9092 \
    --partitions 3 \
    --replication-factor 1
```

### Listar topics
```bash
docker exec fraud_kafka kafka-topics --list --bootstrap-server localhost:9092
```

### Ver mensagens de um topic
```bash
docker exec fraud_kafka kafka-console-consumer \
    --topic transactions \
    --bootstrap-server localhost:9092 \
    --from-beginning \
    --max-messages 5
```

### Verificar quantidade de mensagens
```bash
docker exec fraud_kafka kafka-run-class kafka.tools.GetOffsetShell \
    --broker-list localhost:9092 \
    --topic transactions
```

---

# 6. SHADOWTRAFFIC - GERADOR DE DADOS

## O que √©?
Ferramenta que gera dados fake realistas para testes. Usa arquivos JSON de configura√ß√£o.

## Arquivo de configura√ß√£o: shadowtraffic/transactions.json

### Estrutura de uma transa√ß√£o gerada:
```json
{
  "transaction_id": "uuid",
  "customer_id": "uuid",
  "amount": 150.00,
  "merchant": "Amazon",
  "category": "Eletr√¥nicos",
  "transaction_hour": 14,
  "day_of_week": "Segunda",
  "customer_home_state": "SP",
  "purchase_state": "RJ",
  "purchase_latitude": -22.9068,
  "purchase_longitude": -43.1729,
  "device_latitude": -23.5505,
  "device_longitude": -46.6333,
  "is_fraud": false,
  "had_travel_purchase_last_12m": true,
  "is_first_purchase_in_state": false,
  "transactions_last_24h": 3,
  "avg_transaction_amount_30d": 200.00,
  ...
}
```

### Campos importantes para detec√ß√£o de fraude:
- **customer_home_state vs purchase_state** - Compra fora do estado?
- **device_latitude/longitude vs purchase_latitude/longitude** - GPS batendo?
- **transaction_hour** - Hor√°rio suspeito (madrugada)?
- **transactions_last_24h** - Muitas transa√ß√µes recentes?
- **amount vs avg_transaction_amount_30d** - Valor muito acima da m√©dia?

## Como executar

### Enviar dados de teste (amostra)
```bash
cd /home/ubuntu/Estudos/1_projeto_bank_Fraud_detection_data_pipeline

docker run --rm \
    --network 1_projeto_bank_fraud_detection_data_pipeline_default \
    --env-file shadowtraffic/license.env \
    -v $(pwd)/shadowtraffic:/config \
    shadowtraffic/shadowtraffic:latest \
    --config /config/transactions.json \
    --sample 100
```

### Enviar dados cont√≠nuos (streaming)
```bash
docker run --rm \
    --network 1_projeto_bank_fraud_detection_data_pipeline_default \
    --env-file shadowtraffic/license.env \
    -v $(pwd)/shadowtraffic:/config \
    shadowtraffic/shadowtraffic:latest \
    --config /config/transactions.json
```

### Licen√ßa
Arquivo `shadowtraffic/license.env` cont√©m a licen√ßa trial (v√°lida at√© 2025-12-29).

---

# 7. SPARK - PROCESSAMENTO DE DADOS

## O que √© Apache Spark?
Engine de processamento distribu√≠do para Big Data. Pode processar:
- **Batch**: Dados em lote (arquivo inteiro)
- **Streaming**: Dados em tempo real (micro-batches)

## Arquitetura Spark
```
Driver (Master) ‚Üí Coordena o trabalho
    ‚Üì
Executors (Workers) ‚Üí Executam as tarefas em paralelo
```

## Modos de execu√ß√£o

### Modo Local
```python
spark = SparkSession.builder \
    .appName("MeuJob") \
    .master("local[*]") \  # Usa todos os cores locais
    .getOrCreate()
```

### Modo Cluster
```python
spark = SparkSession.builder \
    .appName("MeuJob") \
    .master("spark://spark-master:7077") \  # Envia para o cluster
    .getOrCreate()
```

## JARs necess√°rios
Spark precisa de JARs extras para conectar com Kafka, MinIO e PostgreSQL:

```
/jars/
‚îú‚îÄ‚îÄ hadoop-aws-3.3.4.jar              # Conex√£o com S3/MinIO
‚îú‚îÄ‚îÄ aws-java-sdk-bundle-1.12.262.jar  # SDK AWS para MinIO
‚îú‚îÄ‚îÄ spark-sql-kafka-0-10_2.12-3.5.3.jar    # Conector Kafka
‚îú‚îÄ‚îÄ kafka-clients-3.5.1.jar           # Cliente Kafka
‚îú‚îÄ‚îÄ commons-pool2-2.11.1.jar          # Pool de conex√µes
‚îú‚îÄ‚îÄ spark-token-provider-kafka-0-10_2.12-3.5.3.jar  # Auth Kafka
‚îî‚îÄ‚îÄ postgresql-42.7.4.jar             # Conector PostgreSQL
```

### Como baixar JARs
```bash
cd /home/ubuntu/Estudos/1_projeto_bank_Fraud_detection_data_pipeline/jars

# Hadoop AWS
wget https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.3.4/hadoop-aws-3.3.4.jar

# AWS SDK
wget https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.12.262/aws-java-sdk-bundle-1.12.262.jar

# Kafka
wget https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.5.3/spark-sql-kafka-0-10_2.12-3.5.3.jar
wget https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/3.5.1/kafka-clients-3.5.1.jar
wget https://repo1.maven.org/maven2/org/apache/commons/commons-pool2/2.11.1/commons-pool2-2.11.1.jar
wget https://repo1.maven.org/maven2/org/apache/spark/spark-token-provider-kafka-0-10_2.12/3.5.3/spark-token-provider-kafka-0-10_2.12-3.5.3.jar

# PostgreSQL
wget https://repo1.maven.org/maven2/org/postgresql/postgresql/42.7.4/postgresql-42.7.4.jar
```

### Como copiar JARs para containers
```bash
for jar in /home/ubuntu/Estudos/1_projeto_bank_Fraud_detection_data_pipeline/jars/*.jar; do
    docker cp "$jar" fraud_spark_master:/jars/
    for i in 1 2 3 4 5; do
        docker cp "$jar" fraud_spark_worker_$i:/jars/
    done
done
```

## Jobs Spark criados

### 1. streaming_bronze.py
- **Fun√ß√£o**: L√™ do Kafka em streaming e salva no MinIO (Bronze)
- **Input**: Kafka topic `transactions`
- **Output**: MinIO `s3a://fraud-data/streaming/bronze/transactions/`

### 2. batch_silver_gold.py
- **Fun√ß√£o**: Processa Bronze ‚Üí Silver ‚Üí Gold em batch
- **Input**: MinIO Bronze
- **Output**: MinIO Silver e Gold + m√©tricas de fraude

### 3. kafka_to_postgres_batch.py
- **Fun√ß√£o**: L√™ do Kafka e salva direto no PostgreSQL
- **Input**: Kafka topic `transactions`
- **Output**: PostgreSQL tabelas `transactions` e `fraud_alerts`

## Como executar jobs Spark

### Formato do comando
```bash
docker exec fraud_spark_master /opt/spark/bin/spark-submit \
    --master spark://spark-master:7077 \
    --jars /jars/jar1.jar,/jars/jar2.jar \
    /jobs/nome_do_job.py
```

### Exemplo completo - Kafka para PostgreSQL
```bash
docker exec fraud_spark_master /opt/spark/bin/spark-submit \
    --master spark://spark-master:7077 \
    --jars /jars/spark-sql-kafka-0-10_2.12-3.5.3.jar,/jars/kafka-clients-3.5.1.jar,/jars/commons-pool2-2.11.1.jar,/jars/spark-token-provider-kafka-0-10_2.12-3.5.3.jar,/jars/postgresql-42.7.4.jar \
    /jobs/kafka_to_postgres_batch.py
```

---

# 8. MINIO - DATA LAKE

## O que √© MinIO?
Object Storage open-source, 100% compat√≠vel com AWS S3.

## Credenciais
```
Endpoint: http://minio:9000
Access Key: minioadmin
Secret Key: minioadmin123@@!!_2
Bucket: fraud-data
```

## Console Web
Acesse: http://localhost:9001 (ou IP do servidor:9001)

## Estrutura de pastas no bucket
```
fraud-data/
‚îú‚îÄ‚îÄ streaming/
‚îÇ   ‚îú‚îÄ‚îÄ bronze/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ transactions/     # Dados brutos do Kafka
‚îÇ   ‚îú‚îÄ‚îÄ silver/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ transactions_batch/  # Dados limpos
‚îÇ   ‚îî‚îÄ‚îÄ gold/
‚îÇ       ‚îú‚îÄ‚îÄ fraud_alerts_batch/  # Alertas de fraude
‚îÇ       ‚îî‚îÄ‚îÄ metrics_batch/       # M√©tricas agregadas
```

## Configura√ß√£o Spark para MinIO
```python
spark = SparkSession.builder \
    .appName("MeuJob") \
    .config("spark.hadoop.fs.s3a.endpoint", "http://minio:9000") \
    .config("spark.hadoop.fs.s3a.access.key", "minioadmin") \
    .config("spark.hadoop.fs.s3a.secret.key", "minioadmin123@@!!_2") \
    .config("spark.hadoop.fs.s3a.path.style.access", "true") \
    .config("spark.hadoop.fs.s3a.impl", "org.apache.hadoop.fs.s3a.S3AFileSystem") \
    .getOrCreate()
```

## IMPORTANTE - Bug do Spark 4.x
Spark 4.x usa AWS SDK v2 que tem bug com endpoints HTTP (n√£o-HTTPS).
**Solu√ß√£o**: Usar Spark 3.5.x com AWS SDK v1.

---

# 9. POSTGRESQL - BANCO ANAL√çTICO

## Credenciais
```
Host: fraud_postgres (dentro do Docker) ou localhost:5432 (fora)
Database: fraud_db
User: fraud_user
Password: fraud_password@@!!_2
```

## Tabelas criadas

### transactions
```sql
CREATE TABLE transactions (
    transaction_id VARCHAR(50) PRIMARY KEY,
    customer_id VARCHAR(50),
    amount DECIMAL(10,2),
    merchant VARCHAR(100),
    category VARCHAR(50),
    fraud_score INTEGER,
    risk_level VARCHAR(20),
    is_fraud BOOLEAN,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

### fraud_alerts
```sql
CREATE TABLE fraud_alerts (
    alert_id SERIAL PRIMARY KEY,
    transaction_id VARCHAR(50),
    customer_id VARCHAR(50),
    amount DECIMAL(10,2),
    merchant VARCHAR(100),
    fraud_score INTEGER,
    risk_level VARCHAR(20),
    is_fraud BOOLEAN,
    customer_home_state VARCHAR(2),
    purchase_state VARCHAR(2),
    alert_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

## Comandos √∫teis

### Conectar ao PostgreSQL
```bash
docker exec -it fraud_postgres psql -U fraud_user -d fraud_db
```

### Ver tabelas
```sql
\dt
```

### Consultas √∫teis
```sql
-- Total de transa√ß√µes
SELECT COUNT(*) FROM transactions;

-- Distribui√ß√£o por risco
SELECT risk_level, COUNT(*) as total 
FROM transactions 
GROUP BY risk_level 
ORDER BY total DESC;

-- Top fraudes
SELECT * FROM transactions 
WHERE risk_level = 'CR√çTICO' 
ORDER BY fraud_score DESC 
LIMIT 10;

-- Alertas recentes
SELECT * FROM fraud_alerts 
ORDER BY alert_at DESC 
LIMIT 10;
```

---

# 10. FLUXO COMPLETO DE DADOS

## Passo a passo do que acontece

### 1. Gera√ß√£o de dados (ShadowTraffic)
```
ShadowTraffic ‚Üí gera JSON ‚Üí envia para Kafka topic "transactions"
```

### 2. Kafka recebe e armazena
```
Kafka recebe mensagem ‚Üí persiste no topic ‚Üí aguarda consumers
```

### 3. Spark l√™ do Kafka
```python
df = spark.read \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "fraud_kafka:9092") \
    .option("subscribe", "transactions") \
    .load()
```

### 4. Parse do JSON
```python
df_parsed = df \
    .selectExpr("CAST(value AS STRING) as json") \
    .select(from_json(col("json"), schema).alias("data")) \
    .select("data.*")
```

### 5. Transforma√ß√µes Silver
```python
# Limpar valores negativos
df = df.withColumn("amount", abs(col("amount")))

# Calcular dist√¢ncia GPS
df = df.withColumn("distance_gps",
    sqrt(pow(col("device_lat") - col("purchase_lat"), 2) +
         pow(col("device_lon") - col("purchase_lon"), 2)))

# Flag de compra fora do estado
df = df.withColumn("is_cross_state",
    col("customer_home_state") != col("purchase_state"))
```

### 6. C√°lculo do Fraud Score (Gold)
```python
df = df.withColumn("fraud_score",
    when(col("is_cross_state"), 15).otherwise(0) +
    when(col("is_night"), 10).otherwise(0) +
    when(col("is_high_value"), 20).otherwise(0) +
    when(col("distance_gps") > 5, 25).otherwise(0) +
    # ... outras regras
)
```

### 7. Classifica√ß√£o de risco
```python
df = df.withColumn("risk_level",
    when(col("fraud_score") >= 70, "CR√çTICO")
    .when(col("fraud_score") >= 50, "ALTO")
    .when(col("fraud_score") >= 30, "M√âDIO")
    .when(col("fraud_score") >= 15, "BAIXO")
    .otherwise("NORMAL"))
```

### 8. Salvamento no PostgreSQL
```python
df.write.jdbc(
    url="jdbc:postgresql://fraud_postgres:5432/fraud_db",
    table="transactions",
    mode="append",
    properties={
        "user": "fraud_user",
        "password": "fraud_password@@!!_2",
        "driver": "org.postgresql.Driver"
    }
)
```

---

# 11. PROBLEMAS ENCONTRADOS E SOLU√á√ïES

## Problema 1: MinIO hostname inv√°lido
**Erro**: `Invalid hostname: fraud_minio`
**Causa**: Underscore (_) n√£o √© permitido em hostnames
**Solu√ß√£o**: Renomear servi√ßo para `minio` (sem underscore)

## Problema 2: Spark 4.x n√£o conecta no MinIO
**Erro**: `software.amazon.awssdk.core.exception.SdkClientException: Unable to execute HTTP request`
**Causa**: AWS SDK v2 (usado no Spark 4.x) tem bug com endpoints HTTP
**Solu√ß√£o**: Usar Spark 3.5.x com AWS SDK v1:
- hadoop-aws-3.3.4.jar
- aws-java-sdk-bundle-1.12.262.jar

## Problema 3: 403 Forbidden no MinIO
**Erro**: `Status Code: 403; Error Code: AccessDenied`
**Causa**: Senha errada do MinIO
**Solu√ß√£o**: Verificar senha no docker-compose.yml e usar a correta

## Problema 4: NoClassDefFoundError Kafka
**Erro**: `NoClassDefFoundError: org/apache/spark/kafka010/KafkaTokenUtil$`
**Causa**: Faltava JAR do token provider
**Solu√ß√£o**: Adicionar `spark-token-provider-kafka-0-10_2.12-3.5.3.jar`

## Problema 5: Streaming travando
**Erro**: Terminal trava sem processar
**Causa**: Streaming com `startingOffsets: latest` n√£o v√™ dados antigos
**Solu√ß√£o**: Usar modo batch ou `startingOffsets: earliest`

## Problema 6: Workers sem JAR
**Erro**: `ClassNotFoundException` nos workers
**Causa**: JARs s√≥ estavam no master
**Solu√ß√£o**: Copiar JARs para todos os workers tamb√©m

---

# 12. COMANDOS √öTEIS

## Docker
```bash
# Ver containers rodando
docker ps

# Ver logs de um container
docker logs fraud_kafka
docker logs -f fraud_spark_master  # -f = follow (tempo real)

# Entrar em um container
docker exec -it fraud_spark_master bash

# Reiniciar container
docker restart fraud_spark_worker_1

# Parar tudo
docker-compose down

# Subir tudo
docker-compose up -d
```

## Kafka
```bash
# Listar topics
docker exec fraud_kafka kafka-topics --list --bootstrap-server localhost:9092

# Ver mensagens
docker exec fraud_kafka kafka-console-consumer \
    --topic transactions \
    --bootstrap-server localhost:9092 \
    --from-beginning --max-messages 5

# Criar topic
docker exec fraud_kafka kafka-topics --create \
    --topic novo_topic \
    --bootstrap-server localhost:9092 \
    --partitions 3
```

## Spark
```bash
# Executar job
docker exec fraud_spark_master /opt/spark/bin/spark-submit \
    --master spark://spark-master:7077 \
    --jars /jars/jar1.jar,/jars/jar2.jar \
    /jobs/meu_job.py

# Ver UI do Spark
# Acesse: http://localhost:8080
```

## PostgreSQL
```bash
# Conectar
docker exec -it fraud_postgres psql -U fraud_user -d fraud_db

# Executar query direta
docker exec fraud_postgres psql -U fraud_user -d fraud_db -c "SELECT COUNT(*) FROM transactions"
```

## MinIO
```bash
# Console web: http://localhost:9001
# Login: minioadmin / minioadmin123@@!!_2
```

---

# 13. COMO REPRODUZIR DO ZERO

## Pr√©-requisitos
- Docker e Docker Compose instalados
- 8GB+ de RAM dispon√≠vel
- Portas livres: 9092, 8080, 9000, 9001, 5432

## Passo 1: Clonar/criar projeto
```bash
mkdir -p ~/fraud-detection-pipeline
cd ~/fraud-detection-pipeline
```

## Passo 2: Criar estrutura de pastas
```bash
mkdir -p spark/jobs jars data shadowtraffic
```

## Passo 3: Criar docker-compose.yml
(copiar do projeto atual)

## Passo 4: Subir infraestrutura
```bash
docker-compose up -d
```

## Passo 5: Baixar JARs
```bash
cd jars
# wget para cada JAR (ver se√ß√£o 7)
```

## Passo 6: Copiar JARs para containers
```bash
for jar in *.jar; do
    docker cp "$jar" fraud_spark_master:/jars/
done
```

## Passo 7: Criar topics Kafka
```bash
docker exec fraud_kafka kafka-topics --create \
    --topic transactions \
    --bootstrap-server localhost:9092 \
    --partitions 5
```

## Passo 8: Configurar ShadowTraffic
- Criar shadowtraffic/transactions.json
- Criar shadowtraffic/license.env

## Passo 9: Criar tabelas PostgreSQL
```bash
docker exec -i fraud_postgres psql -U fraud_user -d fraud_db << 'EOF'
CREATE TABLE IF NOT EXISTS transactions (...);
CREATE TABLE IF NOT EXISTS fraud_alerts (...);
EOF
```

## Passo 10: Enviar dados de teste
```bash
docker run --rm \
    --network $(docker network ls --filter name=fraud -q | head -1) \
    --env-file shadowtraffic/license.env \
    -v $(pwd)/shadowtraffic:/config \
    shadowtraffic/shadowtraffic:latest \
    --config /config/transactions.json \
    --sample 100
```

## Passo 11: Executar pipeline
```bash
docker exec fraud_spark_master /opt/spark/bin/spark-submit \
    --master spark://spark-master:7077 \
    --jars /jars/spark-sql-kafka-0-10_2.12-3.5.3.jar,/jars/kafka-clients-3.5.1.jar,/jars/commons-pool2-2.11.1.jar,/jars/spark-token-provider-kafka-0-10_2.12-3.5.3.jar,/jars/postgresql-42.7.4.jar \
    /jobs/kafka_to_postgres_batch.py
```

## Passo 12: Verificar resultados
```bash
docker exec fraud_postgres psql -U fraud_user -d fraud_db \
    -c "SELECT risk_level, COUNT(*) FROM transactions GROUP BY risk_level"
```

---

# üìù RESUMO FINAL

## O que voc√™ aprendeu:
1. ‚úÖ Arquitetura Medallion (Bronze/Silver/Gold)
2. ‚úÖ Kafka como message broker
3. ‚úÖ Spark para processamento distribu√≠do
4. ‚úÖ MinIO como Data Lake (S3-compatible)
5. ‚úÖ PostgreSQL para dados anal√≠ticos
6. ‚úÖ Docker para orquestra√ß√£o
7. ‚úÖ Detec√ß√£o de fraude com regras de neg√≥cio

## Pr√≥ximos passos sugeridos:
1. üìä Adicionar dashboard (Metabase ou Grafana)
2. ü§ñ Implementar modelo de ML para detec√ß√£o
3. ‚ö° Fazer streaming real funcionar (resolver problema dos JARs)
4. üìà Adicionar mais m√©tricas e agrega√ß√µes
5. üîî Implementar sistema de alertas (email, Slack)

---

**Criado em**: 30/11/2025
**Autor**: Estudo de Data Engineering
**Vers√£o**: 1.0
