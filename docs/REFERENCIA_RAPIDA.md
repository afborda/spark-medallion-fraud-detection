# ğŸš€ REFERÃŠNCIA RÃPIDA - Comandos do Projeto

> **Ãšltima atualizaÃ§Ã£o:** 2025-12-02
> **Pipeline atual:** 51GB de dados brasileiros (51.2M transaÃ§Ãµes)

## ğŸ“ DiretÃ³rio do projeto
```bash
cd /home/ubuntu/Estudos/1_projeto_bank_Fraud_detection_data_pipeline
```

---

## ğŸ“Š MÃ‰TRICAS ATUAIS DO PIPELINE

| MÃ©trica | Valor |
|---------|-------|
| **TransaÃ§Ãµes Raw** | 51,281,996 |
| **TransaÃ§Ãµes Processadas** | 48,445,853 |
| **Dados Raw** | 51 GB (479 arquivos JSON) |
| **Clientes** | 100,000 (Faker pt_BR) |
| **Dispositivos** | 300,102 |
| **Tempo Total** | ~34 min |
| **MinIO Total** | 12 GB |

---

## ğŸ³ DOCKER

### Subir tudo
```bash
docker-compose up -d
```

### Parar tudo
```bash
docker-compose down
```

### Ver containers
```bash
docker ps --format "table {{.Names}}\t{{.Status}}"
```

### Reiniciar workers Spark
```bash
docker restart fraud_spark_worker_1 fraud_spark_worker_2 fraud_spark_worker_3 fraud_spark_worker_4 fraud_spark_worker_5
```

---

## ğŸ“¨ KAFKA

### Listar topics
```bash
docker exec fraud_kafka kafka-topics --list --bootstrap-server localhost:9092
```

### Ver mensagens do topic
```bash
docker exec fraud_kafka kafka-console-consumer \
    --topic transactions \
    --bootstrap-server localhost:9092 \
    --from-beginning \
    --max-messages 5
```

### Criar topic
```bash
docker exec fraud_kafka kafka-topics --create \
    --topic NOME_DO_TOPIC \
    --bootstrap-server localhost:9092 \
    --partitions 5
```

---

## ğŸ­ SHADOWTRAFFIC (Gerar dados)

### Enviar 100 transaÃ§Ãµes (teste)
```bash
docker run --rm \
    --network 1_projeto_bank_fraud_detection_data_pipeline_default \
    --env-file shadowtraffic/license.env \
    -v $(pwd)/shadowtraffic:/config \
    shadowtraffic/shadowtraffic:latest \
    --config /config/transactions.json \
    --sample 100
```

### Streaming contÃ­nuo (produÃ§Ã£o)
```bash
docker run --rm \
    --network 1_projeto_bank_fraud_detection_data_pipeline_default \
    --env-file shadowtraffic/license.env \
    -v $(pwd)/shadowtraffic:/config \
    shadowtraffic/shadowtraffic:latest \
    --config /config/transactions.json
```

---

## âš¡ SPARK

### Executar job Kafka â†’ PostgreSQL
```bash
docker exec fraud_spark_master /opt/spark/bin/spark-submit \
    --master spark://spark-master:7077 \
    --jars /jars/spark-sql-kafka-0-10_2.12-3.5.3.jar,/jars/kafka-clients-3.5.1.jar,/jars/commons-pool2-2.11.1.jar,/jars/spark-token-provider-kafka-0-10_2.12-3.5.3.jar,/jars/postgresql-42.7.4.jar \
    /jobs/kafka_to_postgres_batch.py
```

### Executar job Bronze â†’ Silver â†’ Gold (MinIO)
```bash
docker exec fraud_spark_master /opt/spark/bin/spark-submit \
    --master spark://spark-master:7077 \
    --jars /jars/hadoop-aws-3.3.4.jar,/jars/aws-java-sdk-bundle-1.12.262.jar,/jars/spark-sql-kafka-0-10_2.12-3.5.3.jar,/jars/kafka-clients-3.5.1.jar,/jars/commons-pool2-2.11.1.jar \
    /jobs/batch_silver_gold.py
```

### Ver UI do Spark
```
http://localhost:8080
```

---

## ğŸ˜ POSTGRESQL

### Conectar ao banco
```bash
docker exec -it fraud_postgres psql -U fraud_user -d fraud_db
```

### Ver tabelas
```bash
docker exec fraud_postgres psql -U fraud_user -d fraud_db -c "\dt"
```

### Contar transaÃ§Ãµes
```bash
docker exec fraud_postgres psql -U fraud_user -d fraud_db -c "SELECT COUNT(*) FROM transactions"
```

### Ver distribuiÃ§Ã£o de risco
```bash
docker exec fraud_postgres psql -U fraud_user -d fraud_db -c "
SELECT risk_level, COUNT(*) as total, ROUND(AVG(fraud_score)) as avg_score
FROM transactions 
GROUP BY risk_level 
ORDER BY total DESC"
```

### Ver alertas de fraude
```bash
docker exec fraud_postgres psql -U fraud_user -d fraud_db -c "SELECT COUNT(*) FROM fraud_alerts"
```

### Top transaÃ§Ãµes crÃ­ticas
```bash
docker exec fraud_postgres psql -U fraud_user -d fraud_db -c "
SELECT transaction_id, amount, merchant, fraud_score 
FROM transactions 
WHERE risk_level = 'CRÃTICO' 
ORDER BY fraud_score DESC 
LIMIT 10"
```

---

## ğŸ“¦ MINIO

### Console Web
```
URL: http://localhost:9001
User: minioadmin
Pass: minioadmin123@@!!_2
```

---

## ğŸ”§ DEBUG

### Ver logs do Spark Master
```bash
docker logs fraud_spark_master --tail 50
```

### Ver logs do Kafka
```bash
docker logs fraud_kafka --tail 50
```

### Matar jobs Spark travados
```bash
docker exec fraud_spark_master pkill -f spark-submit
```

### Verificar processos no container
```bash
docker exec fraud_spark_master ps aux
```

---

## ğŸ”„ FLUXO COMPLETO (Copiar e colar)

```bash
# 1. Ir para o diretÃ³rio
cd /home/ubuntu/Estudos/1_projeto_bank_Fraud_detection_data_pipeline

# 2. Verificar containers
docker ps --format "table {{.Names}}\t{{.Status}}" | grep fraud

# 3. Enviar 100 transaÃ§Ãµes para Kafka
docker run --rm \
    --network 1_projeto_bank_fraud_detection_data_pipeline_default \
    --env-file shadowtraffic/license.env \
    -v $(pwd)/shadowtraffic:/config \
    shadowtraffic/shadowtraffic:latest \
    --config /config/transactions.json \
    --sample 100

# 4. Processar e salvar no PostgreSQL
docker exec fraud_spark_master /opt/spark/bin/spark-submit \
    --master spark://spark-master:7077 \
    --jars /jars/spark-sql-kafka-0-10_2.12-3.5.3.jar,/jars/kafka-clients-3.5.1.jar,/jars/commons-pool2-2.11.1.jar,/jars/spark-token-provider-kafka-0-10_2.12-3.5.3.jar,/jars/postgresql-42.7.4.jar \
    /jobs/kafka_to_postgres_batch.py

# 5. Verificar resultados
docker exec fraud_postgres psql -U fraud_user -d fraud_db -c "SELECT risk_level, COUNT(*) FROM transactions GROUP BY risk_level ORDER BY COUNT(*) DESC"
```

---

## ğŸ“‹ CREDENCIAIS

| ServiÃ§o | Host | User | Password |
|---------|------|------|----------|
| PostgreSQL | fraud_postgres:5432 | fraud_user | fraud_password@@!!_2 |
| MinIO | minio:9000 | minioadmin | minioadmin123@@!!_2 |
| Kafka | fraud_kafka:9092 | - | - |
| Spark Master | spark-master:7077 | - | - |

---

## ğŸ“‚ ESTRUTURA DE ARQUIVOS

```
/home/ubuntu/Estudos/1_projeto_bank_Fraud_detection_data_pipeline/
â”œâ”€â”€ docker-compose.yml              # Infraestrutura
â”œâ”€â”€ run_brazilian_pipeline.sh       # ğŸ†• Script para pipeline 51GB
â”œâ”€â”€ spark/jobs/                     # Scripts Spark
â”‚   â”œâ”€â”€ README.md                   # Ãndice principal
â”‚   â”œâ”€â”€ production/                 # ğŸš€ USE ESTES em produÃ§Ã£o!
â”‚   â”‚   â”œâ”€â”€ bronze_brazilian.py    # JSON â†’ MinIO bronze/ (~10min)
â”‚   â”‚   â”œâ”€â”€ silver_brazilian.py    # MinIO bronze/ â†’ silver/ (~13min)
â”‚   â”‚   â”œâ”€â”€ gold_brazilian.py      # MinIO silver/ â†’ gold/ (~11min)
â”‚   â”‚   â””â”€â”€ load_to_postgres.py    # Gold â†’ PostgreSQL
â”‚   â”œâ”€â”€ streaming/                  # ğŸŒŠ Processamento tempo real
â”‚   â”‚   â”œâ”€â”€ streaming_bronze.py
â”‚   â”‚   â”œâ”€â”€ streaming_silver.py
â”‚   â”‚   â”œâ”€â”€ streaming_gold.py
â”‚   â”‚   â””â”€â”€ streaming_to_postgres.py
â”‚   â”œâ”€â”€ utils/                      # ğŸ”§ Debug e validaÃ§Ã£o
â”‚   â”‚   â”œâ”€â”€ check_flags.py
â”‚   â”‚   â””â”€â”€ check_gps.py
â”‚   â””â”€â”€ experimental/               # ğŸ§ª Testes
â”œâ”€â”€ scripts/                        # ğŸ†• Geradores de dados
â”‚   â”œâ”€â”€ generate_parallel.py       # GeraÃ§Ã£o paralela (7 workers)
â”‚   â””â”€â”€ generate_brazilian_data.py # Faker pt_BR
â”œâ”€â”€ jars/                           # JARs do Spark
â”œâ”€â”€ data/raw/                       # 51GB de dados JSON
â””â”€â”€ docs/                           # DocumentaÃ§Ã£o
    â”œâ”€â”€ GUIA_COMPLETO_ESTUDO.md
    â””â”€â”€ REFERENCIA_RAPIDA.md
```

---

## ğŸ‡§ğŸ‡· PIPELINE BRASILEIRO (51GB)

### Gerar dados (jÃ¡ gerados - 51GB)
```bash
python scripts/generate_parallel.py --target-size 51
```

### Executar pipeline completo
```bash
# Bronze (51GB JSON â†’ 5GB Parquet) ~10min
./run_brazilian_pipeline.sh bronze

# Silver (Limpeza e flags) ~13min  
./run_brazilian_pipeline.sh silver

# Gold (AgregaÃ§Ãµes e scoring) ~11min
./run_brazilian_pipeline.sh gold

# PostgreSQL (carregar para dashboards)
./run_brazilian_pipeline.sh postgres

# Ou tudo de uma vez
./run_brazilian_pipeline.sh all
```

### Verificar tamanhos no MinIO
```bash
docker exec fraud_minio mc du -r local/fraud-data/medallion/
```

