# üéì Aprendizado Apache Airflow - Progresso do Abner

> **√öltima atualiza√ß√£o:** 2025-12-05
> **Status:** Em andamento - M√≥dulo 4 CONCLU√çDO ‚úÖ
> **M√©todo:** Ensino linha por linha, digitando c√≥digo, com perguntas de fixa√ß√£o

---

## üìö M√©todo de Ensino Acordado

```
1. CONCEITO   ‚Üí Explica√ß√£o te√≥rica (o que √©, pra que serve)
2. ANALOGIA   ‚Üí Compara√ß√£o com algo do mundo real
3. C√ìDIGO     ‚Üí Aluno digita, professor explica linha por linha
4. EXERC√çCIO  ‚Üí Pergunta para fixar o conhecimento
5. PR√ìXIMO    ‚Üí S√≥ avan√ßa quando entendeu
```

**Regras:**
- N√£o copiar/colar c√≥digo pronto - sempre digitar
- Entender cada linha antes de avan√ßar
- Objetivo: saber fazer sozinho sem IA

---

## ‚úÖ Conceitos J√° Aprendidos

### M√≥dulo 1: Fundamentos (CONCLU√çDO ‚úÖ)

#### 1.1 O que √© Orquestra√ß√£o
- **Aprendido:** Orquestrador ‚â† Processador
- **Analogia:** Chef de cozinha (Airflow) vs Cozinheiros (Spark, Kafka)
- **Pergunta respondida:** "Quem √© o orquestrador no seu projeto hoje?" ‚Üí Resposta: Voc√™ mesmo (executa scripts manualmente)

#### 1.2 O que √© DAG (Directed Acyclic Graph)
- **Aprendido:** 
  - Graph = coisas conectadas
  - Directed = tem dire√ß√£o (setas)
  - Acyclic = sem ciclos (n√£o volta pro in√≠cio)
- **Analogia:** Mapa de metr√¥ com dire√ß√£o √∫nica
- **Pergunta respondida:** "Por que Acyclic?" ‚Üí Resposta: Para evitar loop infinito

#### 1.3 Estrutura de um DAG
- **Aprendido:** 4 partes principais:
  1. IMPORTS - ferramentas do Airflow
  2. DEFAULT_ARGS - configura√ß√µes padr√£o (retries, email)
  3. DEFINI√á√ÉO DO DAG - nome, schedule, tags
  4. TASKS E DEPEND√äNCIAS - o que fazer e em que ordem
- **Analogia:** Receita de bolo (ingredientes, instru√ß√µes, passos)

#### 1.4 Primeiro DAG - Hello World
- **Arquivo criado:** `airflow/dags/hello_world.py`
- **Conceitos aplicados:**
  - `from airflow import DAG`
  - `from airflow.operators.python import PythonOperator`
  - `default_args` com owner, retries, retry_delay, email
  - `with DAG(...) as dag:` para criar o DAG
  - `PythonOperator` para executar fun√ß√µes Python
  - `task_id` e `python_callable`
  - Depend√™ncias com `>>` (task_a >> task_b)

#### 1.5 Paralelismo
- **Aprendido:** Usar lista `[]` para tasks em paralelo
- **Exemplo:** `task_inicio >> [task_hello, task_fim]`
- **Pergunta respondida:** Escolheu C, era B (lista Python)

### M√≥dulo 4: Operadores Avan√ßados (CONCLU√çDO ‚úÖ)

#### 4.1 TaskFlow API
- **Aprendido:** Forma moderna de escrever DAGs com decorators
- **Analogia:** Class Components vs Functional Components no React Native
- **Decorators:** `@dag` e `@task`
- **Vantagem:** Menos c√≥digo, mais leg√≠vel, XCom autom√°tico
- **Arquivo criado:** `airflow/dags/hello_taskflow.py`
- **Pergunta respondida:** "Se esquecer meu_primeiro_taskflow() o que acontece?" ‚Üí B) N√£o aparece na UI

#### 4.2 XCom (Cross-Communication)
- **Aprendido:** Como tasks passam dados entre si
- **TaskFlow:** `return` autom√°tico, par√¢metros autom√°ticos
- **Cl√°ssico:** `ti.xcom_push()` e `ti.xcom_pull()`
- **ti:** Task Instance (inst√¢ncia da task rodando)
- **Limite:** ~48KB por valor (guarda no PostgreSQL)
- **Best practice:** Passar caminhos de arquivo, n√£o dados grandes
- **Pergunta respondida:** "Por que n√£o passar DataFrame 1GB via XCom?" ‚Üí B) Estoura o banco

#### 4.3 Sensors
- **Aprendido:** Tasks que esperam condi√ß√µes
- **Tipos principais:**
  - `FileSensor` ‚Üí arquivos locais
  - `S3KeySensor` ‚Üí arquivos no S3/MinIO
  - `ExternalTaskSensor` ‚Üí outra DAG terminar
  - `HttpSensor` ‚Üí API responder
- **Par√¢metros:** `poke_interval`, `timeout`, `mode='reschedule'`
- **Pergunta respondida:** "Qual sensor para arquivo no MinIO?" ‚Üí B) S3KeySensor

#### 4.4 Branching
- **Aprendido:** Condicionais (if/else) no DAG
- **Operador:** `BranchPythonOperator`
- **Retorno:** Nome do `task_id` que deve executar
- **Pergunta respondida:** "O que BranchPythonOperator retorna?" ‚Üí B) O task_id

---

## üìç Onde Paramos

**Pr√≥ximo passo:** M√≥dulo 5 - Produ√ß√£o (Monitoramento, Health Checks, CI/CD)

**Motiva√ß√£o real:** O streaming job parou por 24h sem ningu√©m perceber!
O Airflow pode monitorar e reiniciar automaticamente.

**Pendente M√≥dulo 5:**
- [ ] DAG de health check (verificar se jobs est√£o rodando)
- [ ] Alertas por email/Slack quando algo falha
- [ ] DAG Factory pattern
- [ ] Testes com pytest

---

## üó∫Ô∏è Roteiro Completo

### M√≥dulo 1: Fundamentos ‚úÖ CONCLU√çDO
- [x] O que √© orquestra√ß√£o
- [x] O que √© DAG
- [x] Estrutura de um DAG
- [x] Primeiro DAG (Hello World)
- [x] Paralelismo com listas

### M√≥dulo 2: Docker e UI ‚úÖ CONCLU√çDO
- [x] Docker Compose para Airflow (arquivo separado)
- [x] Integra√ß√£o com Traefik (dom√≠nio airflow.abnerfonseca.com.br)
- [x] Resolu√ß√£o de problemas (permiss√µes, URL encoding)
- [x] Acessar UI do Airflow
- [x] Executar DAG manualmente
- [x] Ver execu√ß√£o com tasks verdes

### M√≥dulo 3: Integra√ß√£o com Spark ‚úÖ CONCLU√çDO
- [x] BashOperator para executar docker exec
- [x] Docker-in-Docker (montar socket)
- [x] Dockerfile customizado com Docker CLI
- [x] DAG medallion_pipeline completo
- [x] Execu√ß√£o bem sucedida: Bronze ‚Üí Silver ‚Üí Gold ‚Üí Postgres
- [x] Pipeline executou ~65M registros em ~1h40min

### M√≥dulo 4: Operadores Avan√ßados ‚úÖ CONCLU√çDO
- [x] TaskFlow API (@task, @dag) - forma moderna de escrever DAGs
- [x] XCom - passar dados entre tasks (autom√°tico e manual)
- [x] Sensors - esperar arquivos/condi√ß√µes (S3KeySensor, FileSensor)
- [x] Branching - condicionais com BranchPythonOperator
- [x] Criado DAG hello_taskflow.py com ETL exemplo

### M√≥dulo 5: Produ√ß√£o (PR√ìXIMO üëà)
- [ ] Health check DAG (monitorar streaming job)
- [ ] Alertas autom√°ticos
- [ ] DAG Factory pattern
- [ ] Testes com pytest
- [ ] CI/CD

---

## üìù Arquivos Criados

| Arquivo | Status | Descri√ß√£o |
|---------|--------|-----------|
| `airflow/dags/hello_world.py` | ‚úÖ Completo | Primeiro DAG de exemplo |
| `airflow/dags/medallion_pipeline.py` | ‚úÖ Completo | Pipeline Spark completo |
| `airflow/dags/hello_taskflow.py` | ‚úÖ Completo | DAG com TaskFlow API (ETL exemplo) |
| `airflow/APRENDIZADO_AIRFLOW.md` | ‚úÖ Ativo | Este arquivo de progresso |
| `docker-compose.airflow.yml` | ‚úÖ Completo | Docker Compose do Airflow |
| `Dockerfile.airflow` | ‚úÖ Completo | Imagem customizada com Docker CLI |
| `airflow/logs/` | ‚úÖ Criado | Logs do Airflow |
| `airflow/plugins/` | ‚úÖ Criado | Plugins customizados |

---

## üîë Comandos/C√≥digos Importantes Aprendidos

### M√≥dulo 1-3: Forma Cl√°ssica

```python
# Imports b√°sicos
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta

# Default args
default_args = {
    'owner': 'abner',
    'retries': 3,
    'retry_delay': timedelta(minutes=5),
}

# Criar DAG
with DAG(
    dag_id='nome_do_dag',
    default_args=default_args,
    schedule_interval='@daily',
    start_date=datetime(2025, 12, 1),
    catchup=False,
) as dag:

# Criar task
task = PythonOperator(
    task_id='nome_task',
    python_callable=funcao_python,
)

# Depend√™ncias
task_a >> task_b           # sequencial
task_a >> [task_b, task_c] # paralelo
```

### M√≥dulo 4: TaskFlow API (Forma Moderna)

```python
# Imports TaskFlow
from airflow.decorators import dag, task
from datetime import datetime

# DAG com decorator
@dag(
    dag_id='meu_dag',
    start_date=datetime(2025, 1, 1),
    schedule=None,
    catchup=False,
    tags=['exemplo']
)
def minha_dag():
    
    @task
    def extrair():
        dados = {'valores': [1, 2, 3]}
        return dados  # XCom autom√°tico!
    
    @task
    def transformar(dados: dict):  # Recebe automaticamente!
        return {'resultado': sum(dados['valores'])}
    
    @task
    def carregar(dados: dict):
        print(f"Total: {dados['resultado']}")
    
    # Fluxo natural como c√≥digo Python
    dados = extrair()
    transformados = transformar(dados)
    carregar(transformados)

# OBRIGAT√ìRIO: instanciar o DAG
minha_dag()
```

### XCom Manual (Forma Cl√°ssica)

```python
# Push (enviar dados)
def minha_task(**context):
    ti = context['ti']  # TaskInstance
    ti.xcom_push(key='minha_chave', value={'dado': 123})

# Pull (receber dados)
def outra_task(**context):
    ti = context['ti']
    dados = ti.xcom_pull(task_ids='minha_task', key='minha_chave')
```

### Sensors

```python
from airflow.providers.amazon.aws.sensors.s3 import S3KeySensor

esperar_arquivo = S3KeySensor(
    task_id='esperar_csv',
    bucket_name='raw-data',
    bucket_key='transacoes/*.csv',
    aws_conn_id='minio_conn',
    poke_interval=60,      # Verifica a cada 60s
    timeout=3600,          # Timeout 1 hora
    mode='reschedule',     # Libera worker entre checks
)
```

### Branching

```python
from airflow.operators.python import BranchPythonOperator

def escolher_caminho(**context):
    hora = datetime.now().hour
    if hora < 12:
        return 'task_manha'   # Retorna task_id!
    else:
        return 'task_tarde'

branch = BranchPythonOperator(
    task_id='decidir',
    python_callable=escolher_caminho,
)

branch >> [task_manha, task_tarde]
```

```bash
# Comandos Docker para Airflow

# Criar database airflow no PostgreSQL existente
docker exec -it fraud_postgres psql -U fraud_user -d fraud_db -c "CREATE DATABASE airflow;"

# Inicializar Airflow (criar tabelas e usu√°rio admin)
docker compose -f docker-compose.yml -f docker-compose.airflow.yml run --rm airflow-init

# Subir Airflow (webserver + scheduler)
docker compose -f docker-compose.yml -f docker-compose.airflow.yml up -d airflow-webserver airflow-scheduler

# Verificar status
docker ps --filter "name=airflow"

# Ver logs
docker logs fraud_airflow_webserver
docker logs fraud_airflow_scheduler
```

```yaml
# Estrutura do docker-compose.airflow.yml

# Template reutiliz√°vel
x-airflow-common: &airflow-common
  image: apache/airflow:2.10.3
  environment:
    AIRFLOW__CORE__EXECUTOR: LocalExecutor
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://user:pass@host/db

# Servi√ßos
services:
  airflow-init:      # Inicializa√ß√£o (roda uma vez)
  airflow-webserver: # UI (porta 8888)
  airflow-scheduler: # Agendador
```

---

## üêõ Problemas Resolvidos

### 1. Permiss√£o de pastas (Permission denied)
**Problema:** Container Airflow n√£o conseguia escrever em `logs/`
**Solu√ß√£o:** `sudo chmod -R 777 airflow/logs airflow/plugins airflow/dags`

### 2. URL Encoding (senha com caracteres especiais)
**Problema:** Senha `fraud_password@@!!_2` quebrava a URL de conex√£o
**Solu√ß√£o:** URL encoding manual: `@` = `%40`, `!` = `%21`
```
fraud_password@@!!_2 ‚Üí fraud_password%40%40%21%21_2
```

### 3. Docker-in-Docker (executar docker de dentro do Airflow)
**Problema:** Airflow em container n√£o tinha acesso ao Docker do host
**Solu√ß√£o:**
1. Montar socket: `- /var/run/docker.sock:/var/run/docker.sock`
2. Criar Dockerfile.airflow com Docker CLI instalado
3. Rodar como root: `user: "0:0"`

---

## üèÜ Resultados do Pipeline Medallion

**Execu√ß√£o bem sucedida em 2025-12-05:**

| Task | Tempo | Registros |
|------|-------|-----------|
| bronze_ingestion | ~20 min | 51M transa√ß√µes |
| silver_transformation | ~25 min | 51M registros |
| gold_aggregation | ~40 min | M√©tricas + Alertas |
| load_to_postgres | ~15 min | ~65M registros |
| **TOTAL** | **~1h40min** | **Pipeline completo!** |

---

## üí¨ Instru√ß√µes para Pr√≥xima Sess√£o

**Para a IA continuar de onde paramos:**

1. Ler este arquivo primeiro
2. Continuar do "Onde Paramos" (M√≥dulo 3 - Integra√ß√£o com Spark)
3. Manter o m√©todo de ensino (explicar ‚Üí digitar ‚Üí perguntar)
4. Atualizar este arquivo ao final com "salvar"

**Para o aluno:**
- Comando "salvar" = atualiza este documento com o progresso
- N√£o pular etapas
- Perguntar se n√£o entender

---

## üåê Acessos Configurados

| Servi√ßo | URL Local | URL Dom√≠nio |
|---------|-----------|-------------|
| Airflow UI | http://localhost:8888 | https://airflow.abnerfonseca.com.br |
| Login | admin / admin | admin / admin |
